#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é—®é¢˜4ï¼šæœ€ç»ˆä¼˜åŒ–çš„station00 NWPç©ºé—´é™å°ºåº¦åˆ†æž

é’ˆå¯¹æ•°æ®ç‰¹ç‚¹çš„æ ¹æœ¬æ€§æ”¹è¿›ï¼š
1. è§£å†³æ•°æ®ç¨€ç–æ€§é—®é¢˜
2. æ”¹è¿›è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ç­–ç•¥
3. ä¸“æ³¨äºŽå®žé™…å‘ç”µæ—¶æ®µ
4. ä½¿ç”¨æ›´åˆé€‚çš„è¯„ä»·æŒ‡æ ‡
5. å®žçŽ°çœŸæ­£æœ‰æ•ˆçš„ç©ºé—´é™å°ºåº¦
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
from datetime import datetime, timedelta
import matplotlib.dates as mdates
from matplotlib.dates import DateFormatter

warnings.filterwarnings('ignore')

# ä¸­æ–‡å­—ä½“è®¾ç½®
import matplotlib as mpl
font_path = 'C:/Windows/Fonts/simhei.ttf'
mpl.font_manager.fontManager.addfont(font_path)  
mpl.rc('font', family='simhei')
plt.rcParams['axes.unicode_minus'] = False

sns.set_palette("husl")
plt.ioff()

class FinalOptimizedStation00Analysis:
    """æœ€ç»ˆä¼˜åŒ–çš„station00ç©ºé—´é™å°ºåº¦åˆ†æž"""
    
    def __init__(self):
        self.station_id = 'station00'
        self.models = {}
        self.predictions = {}
        self.metrics = {}
        self.capacity = None
        self.train_data = None
        self.test_data = None
        self.scaler = StandardScaler()
        self.generation_threshold = 0.05  # MWï¼Œå®žé™…å‘ç”µé˜ˆå€¼
        
    def load_and_analyze_data(self):
        """åŠ è½½å¹¶æ·±åº¦åˆ†æžæ•°æ®"""
        print(f"ðŸ“Š åŠ è½½å¹¶æ·±åº¦åˆ†æž {self.station_id} æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(f'data/{self.station_id}.csv')
        df['date_time'] = pd.to_datetime(df['date_time'])
        
        # UTCæ—¶é—´è½¬æ¢ä¸ºå½“åœ°æ—¶é—´ï¼ˆä¸­å›½æ—¶åŒº UTC+8ï¼‰
        print(f"  åŽŸå§‹æ—¶é—´ï¼ˆUTCï¼‰: {df['date_time'].min()} åˆ° {df['date_time'].max()}")
        df['date_time'] = df['date_time'] + pd.Timedelta(hours=8)
        print(f"  è½¬æ¢åŽæ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰: {df['date_time'].min()} åˆ° {df['date_time'].max()}")
        
        df = df.sort_values('date_time').reset_index(drop=True)
        
        print(f"  åŽŸå§‹æ•°æ®ç‚¹æ•°: {len(df):,}")
        
        # æ•°æ®æ¸…æ´—
        df['power'] = np.maximum(0, df['power'])
        df['nwp_globalirrad'] = np.maximum(0, df['nwp_globalirrad'])
        df['lmd_totalirrad'] = np.maximum(0, df['lmd_totalirrad'])
        
        # å®¹é‡ä¼°ç®—ï¼ˆä½¿ç”¨æ›´ä¿å®ˆçš„æ–¹æ³•ï¼‰
        self.capacity = df['power'].quantile(0.95)
        print(f"  ä¼°ç®—å®¹é‡: {self.capacity:.3f} MW")
        
        # æ·»åŠ æ—¶é—´ç‰¹å¾
        df['hour'] = df['date_time'].dt.hour
        df['day_of_year'] = df['date_time'].dt.dayofyear
        df['month'] = df['date_time'].dt.month
        df['weekday'] = df['date_time'].dt.weekday
        
        # å¤ªé˜³ä½ç½®è®¡ç®—ï¼ˆåŸºäºŽå½“åœ°æ—¶é—´ï¼‰
        lat = 38.04778
        hour_angle = (df['hour'] + df['date_time'].dt.minute/60 - 12) * 15
        declination = 23.45 * np.sin(np.radians(360 * (284 + df['day_of_year']) / 365.25))
        
        solar_elevation = np.arcsin(
            np.sin(np.radians(declination)) * np.sin(np.radians(lat)) +
            np.cos(np.radians(declination)) * np.cos(np.radians(lat)) * 
            np.cos(np.radians(hour_angle))
        )
        df['solar_elevation'] = np.degrees(solar_elevation)
        df['solar_elevation'] = np.maximum(df['solar_elevation'], 0)
        
        # å®šä¹‰å‘ç”µæ—¶æ®µï¼ˆåŸºäºŽå½“åœ°æ—¶é—´ï¼Œæ›´å‡†ç¡®çš„åˆ¤æ–­ï¼‰
        df['is_generation_time'] = (
            (df['hour'] >= 6) & (df['hour'] <= 18) &  # å½“åœ°æ—¶é—´6-18ç‚¹
            (df['solar_elevation'] > 0) & 
            ((df['nwp_globalirrad'] > 10) | (df['lmd_totalirrad'] > 10))
        ).astype(int)
        
        # å®šä¹‰æœ‰æ•ˆå‘ç”µæ—¶æ®µï¼ˆå®žé™…æœ‰åŠŸçŽ‡è¾“å‡ºï¼‰
        df['is_effective_generation'] = (
            (df['is_generation_time'] == 1) & 
            (df['power'] > self.generation_threshold)
        ).astype(int)
        
        print(f"  å‘ç”µæ—¶æ®µ: {df['is_generation_time'].sum():,} ({df['is_generation_time'].mean()*100:.1f}%)")
        print(f"  æœ‰æ•ˆå‘ç”µæ—¶æ®µ: {df['is_effective_generation'].sum():,} ({df['is_effective_generation'].mean()*100:.1f}%)")
        
        # åˆ†æžæ•°æ®åˆ†å¸ƒ
        self.analyze_data_distribution(df)
        
        return df
    
    def analyze_data_distribution(self, df):
        """åˆ†æžæ•°æ®åˆ†å¸ƒç‰¹å¾"""
        print("\nðŸ“ˆ æ•°æ®åˆ†å¸ƒåˆ†æž:")
        
        # æŒ‰æœˆä»½åˆ†æž
        monthly_stats = df.groupby('month').agg({
            'power': ['mean', 'max', 'count'],
            'is_generation_time': 'sum',
            'is_effective_generation': 'sum'
        }).round(3)
        print("  æœˆä»½ç»Ÿè®¡:")
        print(monthly_stats)
        
        # æŒ‰å°æ—¶åˆ†æž
        hourly_stats = df.groupby('hour').agg({
            'power': ['mean', 'max'],
            'is_generation_time': 'sum',
            'is_effective_generation': 'sum'
        }).round(3)
        print("\n  å°æ—¶ç»Ÿè®¡ï¼ˆå‰10å°æ—¶ï¼‰:")
        print(hourly_stats.head(10))
        
        # ç›¸å…³æ€§åˆ†æž
        generation_data = df[df['is_generation_time'] == 1]
        if len(generation_data) > 0:
            corr_power_nwp = generation_data['power'].corr(generation_data['nwp_globalirrad'])
            corr_power_lmd = generation_data['power'].corr(generation_data['lmd_totalirrad'])
            print(f"\n  å‘ç”µæ—¶æ®µç›¸å…³æ€§:")
            print(f"    åŠŸçŽ‡ vs NWPè¾å°„: {corr_power_nwp:.4f}")
            print(f"    åŠŸçŽ‡ vs LMDè¾å°„: {corr_power_lmd:.4f}")
    
    def create_advanced_features(self, df):
        """åˆ›å»ºé«˜çº§ç‰¹å¾å·¥ç¨‹"""
        print("ðŸ”§ åˆ›å»ºé«˜çº§ç‰¹å¾å·¥ç¨‹...")
        
        data = df.copy()
        
        # 1. åŸºç¡€è¾å°„ç‰¹å¾
        data['nwp_diffuse_irrad'] = data['nwp_globalirrad'] - data['nwp_directirrad']
        data['lmd_direct_irrad'] = data['lmd_totalirrad'] - data['lmd_diffuseirrad']
        
        # 2. è¾å°„æ¯”ä¾‹å’Œè´¨é‡æŒ‡æ ‡
        data['nwp_diffuse_ratio'] = data['nwp_diffuse_irrad'] / (data['nwp_globalirrad'] + 1e-6)
        data['lmd_diffuse_ratio'] = data['lmd_diffuseirrad'] / (data['lmd_totalirrad'] + 1e-6)
        data['clearness_index_nwp'] = data['nwp_globalirrad'] / (1361 * np.sin(np.radians(data['solar_elevation'])) + 1e-6)
        data['clearness_index_lmd'] = data['lmd_totalirrad'] / (1361 * np.sin(np.radians(data['solar_elevation'])) + 1e-6)
        
        # é™åˆ¶æ¸…æ™°åº¦æŒ‡æ•°åœ¨åˆç†èŒƒå›´å†…
        data['clearness_index_nwp'] = np.clip(data['clearness_index_nwp'], 0, 1.2)
        data['clearness_index_lmd'] = np.clip(data['clearness_index_lmd'], 0, 1.2)
        
        # 3. æ¸©åº¦ç‰¹å¾
        data['nwp_temp_celsius'] = data['nwp_temperature'] - 273.15
        data['temp_diff_nwp_lmd'] = data['nwp_temp_celsius'] - data['lmd_temperature']
        data['temp_efficiency_factor'] = 1 - 0.004 * (data['nwp_temp_celsius'] - 25)  # æ¸©åº¦æ•ˆçŽ‡å› å­
        
        # 4. æ”¹è¿›çš„ç©ºé—´é™å°ºåº¦ç‰¹å¾
        # åŸºäºŽåœ°ç†å’Œæ°”å€™çš„ä¿®æ­£å› å­
        lat_correction = 1 + (38.04778 - 38.0) * 0.02  # çº¬åº¦ä¿®æ­£
        lon_correction = 1 + (114.95 - 115.0) * 0.01   # ç»åº¦ä¿®æ­£
        elevation_correction = 1.02  # å‡è®¾çš„æµ·æ‹”ä¿®æ­£
        
        # å­£èŠ‚æ€§ä¿®æ­£
        season_correction = 1 + 0.1 * np.sin(2 * np.pi * data['day_of_year'] / 365.25)
        
        # ç»¼åˆä¿®æ­£å› å­
        spatial_correction = lat_correction * lon_correction * elevation_correction * season_correction
        
        data['nwp_globalirrad_downscaled'] = data['nwp_globalirrad'] * spatial_correction
        data['nwp_directirrad_downscaled'] = data['nwp_directirrad'] * spatial_correction
        
        # 5. æ—¶é—´ç‰¹å¾
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
        data['day_sin'] = np.sin(2 * np.pi * data['day_of_year'] / 365.25)
        data['day_cos'] = np.cos(2 * np.pi * data['day_of_year'] / 365.25)
        data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
        data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)
        
        # 6. ç†è®ºåŠŸçŽ‡è®¡ç®—
        data['theoretical_power_nwp'] = (
            data['nwp_globalirrad'] / 1000 * self.capacity * 
            data['temp_efficiency_factor'] * 
            np.sin(np.radians(data['solar_elevation']))
        )
        data['theoretical_power_lmd'] = (
            data['lmd_totalirrad'] / 1000 * self.capacity * 
            data['temp_efficiency_factor'] * 
            np.sin(np.radians(data['solar_elevation']))
        )
        data['theoretical_power_downscaled'] = (
            data['nwp_globalirrad_downscaled'] / 1000 * self.capacity * 
            data['temp_efficiency_factor'] * 
            np.sin(np.radians(data['solar_elevation']))
        )
        
        # ç¡®ä¿ç†è®ºåŠŸçŽ‡éžè´Ÿ
        for col in ['theoretical_power_nwp', 'theoretical_power_lmd', 'theoretical_power_downscaled']:
            data[col] = np.maximum(0, data[col])
        
        # 7. æ•°æ®æºå¯¹æ¯”ç‰¹å¾
        data['irrad_ratio_nwp_lmd'] = data['nwp_globalirrad'] / (data['lmd_totalirrad'] + 1e-6)
        data['irrad_diff_nwp_lmd'] = data['nwp_globalirrad'] - data['lmd_totalirrad']
        data['irrad_ratio_downscaled_lmd'] = data['nwp_globalirrad_downscaled'] / (data['lmd_totalirrad'] + 1e-6)
        
        # 8. æ»žåŽç‰¹å¾ï¼ˆåªåœ¨å‘ç”µæ—¶æ®µï¼‰
        for lag in [1, 4, 12, 24]:  # 15åˆ†é’Ÿåˆ°6å°æ—¶
            data[f'power_lag_{lag}'] = data['power'].shift(lag)
            data[f'irrad_nwp_lag_{lag}'] = data['nwp_globalirrad'].shift(lag)
            data[f'irrad_lmd_lag_{lag}'] = data['lmd_totalirrad'].shift(lag)
        
        # 9. æ»‘åŠ¨çª—å£ç‰¹å¾
        for window in [4, 8, 12]:  # 1-3å°æ—¶çª—å£
            data[f'power_rolling_mean_{window}'] = data['power'].rolling(window=window, min_periods=1).mean()
            data[f'irrad_nwp_rolling_mean_{window}'] = data['nwp_globalirrad'].rolling(window=window, min_periods=1).mean()
            data[f'irrad_lmd_rolling_mean_{window}'] = data['lmd_totalirrad'].rolling(window=window, min_periods=1).mean()
        
        # 10. å¤©æ°”æ¨¡å¼ç‰¹å¾
        data['weather_clear'] = ((data['nwp_globalirrad'] > 600) & (data['nwp_diffuse_ratio'] < 0.3)).astype(int)
        data['weather_partly_cloudy'] = ((data['nwp_globalirrad'] > 200) & (data['nwp_globalirrad'] <= 600)).astype(int)
        data['weather_cloudy'] = ((data['nwp_globalirrad'] > 50) & (data['nwp_globalirrad'] <= 200)).astype(int)
        
        # å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
        data = data.fillna(0)
        data = data.replace([np.inf, -np.inf], 0)
        
        print(f"  æ€»ç‰¹å¾æ•°é‡: {len(data.columns) - 2}")
        
        return data
    
    def smart_data_split(self, data, test_ratio=0.2):
        """æ™ºèƒ½æ•°æ®åˆ†å‰²ç­–ç•¥"""
        print("ðŸ“Š æ™ºèƒ½æ•°æ®åˆ†å‰²...")
        
        # ç¡®ä¿æµ‹è¯•é›†åŒ…å«è¶³å¤Ÿçš„å‘ç”µæ•°æ®
        generation_data = data[data['is_generation_time'] == 1].copy()
        non_generation_data = data[data['is_generation_time'] == 0].copy()
        
        print(f"  å‘ç”µæ—¶æ®µæ•°æ®: {len(generation_data):,}")
        print(f"  éžå‘ç”µæ—¶æ®µæ•°æ®: {len(non_generation_data):,}")
        
        # æŒ‰æ—¶é—´åˆ†å‰²å‘ç”µæ•°æ®
        n_test_generation = int(len(generation_data) * test_ratio)
        train_generation = generation_data[:-n_test_generation]
        test_generation = generation_data[-n_test_generation:]
        
        # æŒ‰æ—¶é—´åˆ†å‰²éžå‘ç”µæ•°æ®
        n_test_non_generation = int(len(non_generation_data) * test_ratio)
        train_non_generation = non_generation_data[:-n_test_non_generation]
        test_non_generation = non_generation_data[-n_test_non_generation:]
        
        # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        train_data = pd.concat([train_generation, train_non_generation]).sort_values('date_time')
        test_data = pd.concat([test_generation, test_non_generation]).sort_values('date_time')
        
        print(f"  è®­ç»ƒé›†: {len(train_data):,} (å‘ç”µ: {len(train_generation):,}, éžå‘ç”µ: {len(train_non_generation):,})")
        print(f"  æµ‹è¯•é›†: {len(test_data):,} (å‘ç”µ: {len(test_generation):,}, éžå‘ç”µ: {len(test_non_generation):,})")
        
        return train_data, test_data
    
    def train_optimized_models(self, train_data):
        """è®­ç»ƒä¼˜åŒ–æ¨¡åž‹"""
        print("ðŸš€ è®­ç»ƒä¼˜åŒ–é¢„æµ‹æ¨¡åž‹...")
        
        # åªä½¿ç”¨å‘ç”µæ—¶æ®µæ•°æ®è®­ç»ƒ
        generation_train = train_data[train_data['is_generation_time'] == 1].copy()
        
        if len(generation_train) < 100:
            print("âš ï¸  å‘ç”µæ—¶æ®µæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ•°æ®")
            generation_train = train_data.copy()
        
        print(f"  å®žé™…è®­ç»ƒæ•°æ®: {len(generation_train):,}")
        
        # ç‰¹å¾é€‰æ‹©
        feature_cols = [col for col in generation_train.columns 
                       if col not in ['date_time', 'power', 'is_generation_time', 'is_effective_generation']]
        
        X_train = generation_train[feature_cols]
        y_train = generation_train['power']
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)
        
        self.feature_names = feature_cols
        
        # 1. åŸºçº¿æ¨¡åž‹ï¼ˆç®€å•ç‰¹å¾ï¼‰
        print("  è®­ç»ƒåŸºçº¿æ¨¡åž‹...")
        basic_features = ['nwp_globalirrad', 'lmd_totalirrad', 'solar_elevation', 
                         'hour_sin', 'hour_cos', 'theoretical_power_nwp']
        basic_features = [f for f in basic_features if f in feature_cols]
        
        if basic_features:
            X_basic = X_train_scaled[basic_features]
            self.models['baseline'] = Ridge(alpha=1.0)
            self.models['baseline'].fit(X_basic, y_train)
        
        # 2. åŽŸå§‹NWPæ¨¡åž‹
        print("  è®­ç»ƒåŽŸå§‹NWPæ¨¡åž‹...")
        original_features = [col for col in feature_cols 
                           if not 'downscaled' in col and not 'lag' in col]
        X_original = X_train_scaled[original_features]
        
        self.models['original_nwp'] = xgb.XGBRegressor(
            objective='reg:squarederror',
            max_depth=6,
            learning_rate=0.1,
            n_estimators=200,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        self.models['original_nwp'].fit(X_original, y_train)
        
        # 3. é™å°ºåº¦NWPæ¨¡åž‹
        print("  è®­ç»ƒé™å°ºåº¦NWPæ¨¡åž‹...")
        downscaled_features = [col for col in feature_cols 
                             if 'downscaled' in col or not col.startswith('nwp_') or 
                             col.startswith('theoretical_power_downscaled')]
        X_downscaled = X_train_scaled[downscaled_features]
        
        self.models['downscaled_nwp'] = xgb.XGBRegressor(
            objective='reg:squarederror',
            max_depth=6,
            learning_rate=0.1,
            n_estimators=200,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        self.models['downscaled_nwp'].fit(X_downscaled, y_train)
        
        # 4. æ··åˆæ¨¡åž‹
        print("  è®­ç»ƒæ··åˆæ¨¡åž‹...")
        self.models['hybrid'] = xgb.XGBRegressor(
            objective='reg:squarederror',
            max_depth=8,
            learning_rate=0.08,
            n_estimators=300,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=42,
            n_jobs=-1
        )
        self.models['hybrid'].fit(X_train_scaled, y_train)
        
        # 5. éšæœºæ£®æž—æ¨¡åž‹
        print("  è®­ç»ƒéšæœºæ£®æž—æ¨¡åž‹...")
        self.models['random_forest'] = RandomForestRegressor(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        self.models['random_forest'].fit(X_train_scaled, y_train)
        
        # æ‰“å°è®­ç»ƒæ€§èƒ½
        print("\n  è®­ç»ƒé›†æ€§èƒ½:")
        for model_name, model in self.models.items():
            if model_name == 'baseline' and basic_features:
                train_pred = model.predict(X_train_scaled[basic_features])
            elif model_name == 'original_nwp':
                train_pred = model.predict(X_train_scaled[original_features])
            elif model_name == 'downscaled_nwp':
                train_pred = model.predict(X_train_scaled[downscaled_features])
            else:
                train_pred = model.predict(X_train_scaled)
            
            train_r2 = r2_score(y_train, train_pred)
            train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
            print(f"    {model_name}: R^2 = {train_r2:.4f}, RMSE = {train_rmse:.4f}")
    
    def predict_with_postprocessing(self, test_data):
        """å¸¦åŽå¤„ç†çš„é¢„æµ‹"""
        print("ðŸ”® è¿›è¡Œé¢„æµ‹...")
        
        predictions = {}
        feature_cols = self.feature_names
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        X_test = test_data[feature_cols]
        X_test_scaled = self.scaler.transform(X_test)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)
        
        for model_name, model in self.models.items():
            print(f"  {model_name} é¢„æµ‹ä¸­...")
            
            # é€‰æ‹©å¯¹åº”çš„ç‰¹å¾
            if model_name == 'baseline':
                basic_features = ['nwp_globalirrad', 'lmd_totalirrad', 'solar_elevation', 
                                'hour_sin', 'hour_cos', 'theoretical_power_nwp']
                basic_features = [f for f in basic_features if f in feature_cols]
                if basic_features:
                    X_model = X_test_scaled[basic_features]
                else:
                    continue
            elif model_name == 'original_nwp':
                original_features = [col for col in feature_cols 
                                   if not 'downscaled' in col and not 'lag' in col]
                X_model = X_test_scaled[original_features]
            elif model_name == 'downscaled_nwp':
                downscaled_features = [col for col in feature_cols 
                                     if 'downscaled' in col or not col.startswith('nwp_') or 
                                     col.startswith('theoretical_power_downscaled')]
                X_model = X_test_scaled[downscaled_features]
            else:
                X_model = X_test_scaled
            
            # é¢„æµ‹
            pred = model.predict(X_model)
            
            # åŽå¤„ç†
            pred = np.maximum(0, pred)  # ç¡®ä¿éžè´Ÿ
            pred = np.minimum(pred, self.capacity * 1.1)  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            
            # éžå‘ç”µæ—¶æ®µè®¾ä¸º0
            non_generation_mask = test_data['is_generation_time'] == 0
            pred[non_generation_mask] = 0
            
            predictions[model_name] = pred
        
        return predictions
    
    def calculate_comprehensive_metrics(self, actual, predicted, test_data, model_name=""):
        """è®¡ç®—ç»¼åˆè¯„ä»·æŒ‡æ ‡"""
        # å…¨æ•°æ®é›†æŒ‡æ ‡
        all_r2 = r2_score(actual, predicted)
        all_rmse = np.sqrt(mean_squared_error(actual, predicted))
        all_mae = mean_absolute_error(actual, predicted)
        
        # åªè€ƒè™‘å‘ç”µæ—¶æ®µ
        generation_mask = test_data['is_generation_time'] == 1
        if np.sum(generation_mask) > 0:
            actual_gen = actual[generation_mask]
            predicted_gen = predicted[generation_mask]
            
            gen_r2 = r2_score(actual_gen, predicted_gen)
            gen_rmse = np.sqrt(mean_squared_error(actual_gen, predicted_gen))
            gen_mae = mean_absolute_error(actual_gen, predicted_gen)
            gen_corr = np.corrcoef(actual_gen, predicted_gen)[0, 1] if len(actual_gen) > 1 else 0
            
            # å½’ä¸€åŒ–æŒ‡æ ‡
            gen_nrmse = gen_rmse / (actual_gen.max() - actual_gen.min() + 1e-6) * 100
            gen_mape = np.mean(np.abs((actual_gen - predicted_gen) / (actual_gen + 1e-6))) * 100
        else:
            gen_r2 = gen_rmse = gen_mae = gen_corr = gen_nrmse = gen_mape = 0
        
        # åªè€ƒè™‘æœ‰æ•ˆå‘ç”µæ—¶æ®µï¼ˆåŠŸçŽ‡>é˜ˆå€¼ï¼‰
        effective_mask = (test_data['is_effective_generation'] == 1) | (predicted > self.generation_threshold)
        if np.sum(effective_mask) > 0:
            actual_eff = actual[effective_mask]
            predicted_eff = predicted[effective_mask]
            
            eff_r2 = r2_score(actual_eff, predicted_eff)
            eff_rmse = np.sqrt(mean_squared_error(actual_eff, predicted_eff))
            eff_mae = mean_absolute_error(actual_eff, predicted_eff)
        else:
            eff_r2 = eff_rmse = eff_mae = 0
        
        return {
            'All_R2': all_r2,
            'All_RMSE': all_rmse,
            'All_MAE': all_mae,
            'Generation_R2': gen_r2,
            'Generation_RMSE': gen_rmse,
            'Generation_MAE': gen_mae,
            'Generation_Correlation': gen_corr,
            'Generation_NRMSE': gen_nrmse,
            'Generation_MAPE': gen_mape,
            'Effective_R2': eff_r2,
            'Effective_RMSE': eff_rmse,
            'Effective_MAE': eff_mae,
            'Generation_Sample_Count': np.sum(generation_mask),
            'Effective_Sample_Count': np.sum(effective_mask),
            'Model_Name': model_name
        }
    
    def analyze_downscaling_effectiveness(self):
        """åˆ†æžé™å°ºåº¦æœ‰æ•ˆæ€§"""
        print("ðŸ“ˆ åˆ†æžç©ºé—´é™å°ºåº¦æŠ€æœ¯çš„æœ‰æ•ˆæ€§...")
        
        actual = self.test_data['power'].values
        
        # è®¡ç®—å„æ¨¡åž‹æŒ‡æ ‡
        for model_name, predictions in self.predictions.items():
            metrics = self.calculate_comprehensive_metrics(actual, predictions, self.test_data, model_name)
            self.metrics[model_name] = metrics
        
        # æ•ˆæžœå¯¹æ¯”
        print("\n" + "="*100)
        print("ðŸ“Š æ¨¡åž‹æ€§èƒ½å¯¹æ¯”")
        print("="*100)
        
        for model_name, metrics in self.metrics.items():
            print(f"\n{model_name.upper()}:")
            print(f"  å…¨æ•°æ®é›† - R^2: {metrics['All_R2']:.4f}, RMSE: {metrics['All_RMSE']:.4f}")
            print(f"  å‘ç”µæ—¶æ®µ - R^2: {metrics['Generation_R2']:.4f}, RMSE: {metrics['Generation_RMSE']:.4f}, ç›¸å…³ç³»æ•°: {metrics['Generation_Correlation']:.4f}")
            print(f"  æœ‰æ•ˆå‘ç”µ - R^2: {metrics['Effective_R2']:.4f}, RMSE: {metrics['Effective_RMSE']:.4f}")
            print(f"  æ ·æœ¬æ•°é‡ - å‘ç”µ: {metrics['Generation_Sample_Count']}, æœ‰æ•ˆ: {metrics['Effective_Sample_Count']}")
        
        # é™å°ºåº¦æœ‰æ•ˆæ€§åˆ†æž
        if 'original_nwp' in self.metrics and 'downscaled_nwp' in self.metrics:
            orig = self.metrics['original_nwp']
            down = self.metrics['downscaled_nwp']
            
            # åŸºäºŽå‘ç”µæ—¶æ®µçš„æ”¹å–„
            gen_r2_improvement = down['Generation_R2'] - orig['Generation_R2']
            gen_rmse_improvement = (orig['Generation_RMSE'] - down['Generation_RMSE']) / orig['Generation_RMSE'] * 100
            
            print(f"\n{'='*100}")
            print("ðŸŽ¯ ç©ºé—´é™å°ºåº¦æŠ€æœ¯æœ‰æ•ˆæ€§åˆ†æž")
            print("="*100)
            print(f"å‘ç”µæ—¶æ®µR^2æ”¹å–„: {gen_r2_improvement:.4f}")
            print(f"å‘ç”µæ—¶æ®µRMSEæ”¹å–„: {gen_rmse_improvement:.2f}%")
            
            # æœ‰æ•ˆæ€§ç­‰çº§
            if gen_r2_improvement > 0.05 and gen_rmse_improvement > 5:
                effectiveness = "æ˜¾è‘—æœ‰æ•ˆ"
            elif gen_r2_improvement > 0.02 and gen_rmse_improvement > 2:
                effectiveness = "æœ‰æ•ˆ"
            elif gen_r2_improvement > 0:
                effectiveness = "è½»å¾®æœ‰æ•ˆ"
            else:
                effectiveness = "æ— æ•ˆæˆ–è´Ÿé¢å½±å“"
            
            print(f"æœ‰æ•ˆæ€§ç­‰çº§: {effectiveness}")
            
            return effectiveness
        
        return "æ— æ³•è¯„ä¼°"
    
    def create_comprehensive_visualizations(self):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨"""
        print("ðŸ“Š ç”Ÿæˆä¼˜åŒ–çš„ç»¼åˆå¯è§†åŒ–åˆ†æžå›¾è¡¨...")
        
        # åˆ›å»ºåˆ†ç±»æ–‡ä»¶å¤¹ç»“æž„
        base_dir = Path("results/figures")
        folders = {
            'time_series': base_dir / "æ—¶é—´åºåˆ—åˆ†æž",
            'prediction': base_dir / "é¢„æµ‹æ•ˆæžœåˆ†æž", 
            'performance': base_dir / "æ€§èƒ½å¯¹æ¯”åˆ†æž",
            'data_analysis': base_dir / "æ•°æ®åˆ†å¸ƒåˆ†æž",
            'downscaling': base_dir / "é™å°ºåº¦æ•ˆæžœåˆ†æž"
        }
        
        for folder in folders.values():
            folder.mkdir(parents=True, exist_ok=True)
        
        # 1. æ—¶é—´åºåˆ—åˆ†æž
        self.plot_detailed_time_series(folders['time_series'])
        
        # 2. é¢„æµ‹æ•ˆæžœåˆ†æž
        self.plot_prediction_analysis(folders['prediction'])
        
        # 3. æ€§èƒ½å¯¹æ¯”åˆ†æž
        self.plot_comprehensive_performance(folders['performance'])
        
        # 4. æ•°æ®åˆ†å¸ƒåˆ†æž
        self.plot_data_distribution_analysis(folders['data_analysis'])
        
        # 5. é™å°ºåº¦æ•ˆæžœåˆ†æž
        self.plot_downscaling_analysis(folders['downscaling'])
        
        print("âœ… ä¼˜åŒ–çš„ç»¼åˆå¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
        print("ðŸ“ å›¾è¡¨å·²æŒ‰ç±»åˆ«ä¿å­˜åˆ°ä¸åŒæ–‡ä»¶å¤¹ä¸­")
    
    def plot_data_distribution_analysis(self, folder):
        """ç»˜åˆ¶æ•°æ®åˆ†å¸ƒåˆ†æžå›¾"""
        print("ðŸ“Š ç»˜åˆ¶æ•°æ®åˆ†å¸ƒåˆ†æž...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'{self.station_id} æ•°æ®åˆ†å¸ƒä¸Žç‰¹å¾åˆ†æž', fontsize=16, fontweight='bold')
        
        # 1. åŠŸçŽ‡åˆ†å¸ƒç›´æ–¹å›¾
        ax1 = axes[0, 0]
        power_data = self.test_data['power']
        generation_power = power_data[power_data > 0]
        
        ax1.hist(generation_power, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_title('å‘ç”µåŠŸçŽ‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax1.set_xlabel('åŠŸçŽ‡ (MW)', fontsize=12)
        ax1.set_ylabel('é¢‘æ¬¡', fontsize=12)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats_text = (f'æœ€å¤§å€¼: {generation_power.max():.3f} MW\n'
                     f'å¹³å‡å€¼: {generation_power.mean():.3f} MW\n'
                     f'ä¸­ä½æ•°: {generation_power.median():.3f} MW\n'
                     f'æ ‡å‡†å·®: {generation_power.std():.3f} MW')
        ax1.text(0.7, 0.7, stats_text, transform=ax1.transAxes, 
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=10)
        
        # 2. æ—¶é—´åˆ†å¸ƒåˆ†æž
        ax2 = axes[0, 1]
        hourly_power = self.test_data.groupby(self.test_data['date_time'].dt.hour)['power'].mean()
        
        ax2.bar(hourly_power.index, hourly_power.values, alpha=0.7, color='lightgreen', edgecolor='black')
        ax2.set_title('å°æ—¶å¹³å‡åŠŸçŽ‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax2.set_xlabel('å°æ—¶', fontsize=12)
        ax2.set_ylabel('å¹³å‡åŠŸçŽ‡ (MW)', fontsize=12)
        ax2.grid(True, alpha=0.3, axis='y')
        ax2.set_xticks(range(0, 24, 2))
        
        # 3. å‘ç”µæ—¶æ®µæ¯”ä¾‹åˆ†æž
        ax3 = axes[1, 0]
        generation_stats = {
            'éžå‘ç”µæ—¶æ®µ': (self.test_data['is_generation_time'] == 0).sum(),
            'å‘ç”µæ—¶æ®µ': (self.test_data['is_generation_time'] == 1).sum(),
            'æœ‰æ•ˆå‘ç”µæ—¶æ®µ': (self.test_data['is_effective_generation'] == 1).sum()
        }
        
        colors_pie = ['lightcoral', 'lightblue', 'lightgreen']
        wedges, texts, autotexts = ax3.pie(generation_stats.values(), 
                                          labels=generation_stats.keys(),
                                          autopct='%1.1f%%',
                                          colors=colors_pie,
                                          startangle=90)
        ax3.set_title('å‘ç”µæ—¶æ®µåˆ†å¸ƒ', fontsize=14, fontweight='bold')
        
        # ç¾ŽåŒ–é¥¼å›¾
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
            autotext.set_fontsize(12)
        
        # 4. æ¨¡åž‹é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
        ax4 = axes[1, 1]
        if 'original_nwp' in self.predictions:
            actual = self.test_data['power'].values
            generation_mask = self.test_data['is_generation_time'] == 1
            
            if generation_mask.sum() > 0:
                actual_gen = actual[generation_mask]
                pred_gen = self.predictions['original_nwp'][generation_mask]
                errors = pred_gen - actual_gen
                
                ax4.hist(errors, bins=30, alpha=0.7, color='orange', edgecolor='black')
                ax4.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒï¼ˆåŽŸå§‹NWPï¼‰', fontsize=14, fontweight='bold')
                ax4.set_xlabel('é¢„æµ‹è¯¯å·® (MW)', fontsize=12)
                ax4.set_ylabel('é¢‘æ¬¡', fontsize=12)
                ax4.grid(True, alpha=0.3)
                ax4.axvline(x=0, color='red', linestyle='--', alpha=0.8, label='é›¶è¯¯å·®çº¿')
                
                # æ·»åŠ è¯¯å·®ç»Ÿè®¡
                error_stats = (f'RMSE: {np.sqrt(np.mean(errors**2)):.3f} MW\n'
                              f'MAE: {np.mean(np.abs(errors)):.3f} MW\n'
                              f'åå·®: {np.mean(errors):.3f} MW\n'
                              f'æ ‡å‡†å·®: {np.std(errors):.3f} MW')
                ax4.text(0.05, 0.95, error_stats, transform=ax4.transAxes, va='top',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=10)
                ax4.legend()
        
        plt.tight_layout()
        # ä¿å­˜å¤šä¸ªç‰ˆæœ¬
        plt.savefig(folder / f'{self.station_id}_æ•°æ®åˆ†å¸ƒåˆ†æž.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.savefig(folder / f'{self.station_id}_data_distribution_analysis.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
    
    def plot_comprehensive_performance(self, folder):
        """ç»˜åˆ¶ç»¼åˆæ€§èƒ½å¯¹æ¯”"""
        print("ðŸ“Š ç»˜åˆ¶ä¼˜åŒ–çš„ç»¼åˆæ€§èƒ½å¯¹æ¯”...")
        
        fig, axes = plt.subplots(2, 3, figsize=(24, 16))
        fig.suptitle(f'{self.station_id} æ¨¡åž‹ç»¼åˆæ€§èƒ½å¯¹æ¯”åˆ†æž', fontsize=20, fontweight='bold', y=0.98)
        
        models = list(self.metrics.keys())
        # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ–¹æ¡ˆ
        colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#592E83'][:len(models)]
        
        # 1. å‘ç”µæ—¶æ®µR^2å¯¹æ¯”ï¼ˆæœ€é‡è¦çš„æŒ‡æ ‡ï¼‰
        ax1 = axes[0, 0]
        gen_r2_values = [self.metrics[m]['Generation_R2'] for m in models]
        bars = ax1.bar(models, gen_r2_values, alpha=0.8, color=colors, edgecolor='black', linewidth=1.5)
        ax1.set_title('å‘ç”µæ—¶æ®µR^2å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
        ax1.set_ylabel('R^2 å€¼', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3, axis='y')
        ax1.set_ylim(0, 1.0)
        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=12)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, gen_r2_values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        # æ·»åŠ ä¼˜ç§€çº¿
        ax1.axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='ä¼˜ç§€çº¿(0.9)')
        ax1.legend()
        
        # 2. å‘ç”µæ—¶æ®µRMSEå¯¹æ¯”
        ax2 = axes[0, 1]
        gen_rmse_values = [self.metrics[m]['Generation_RMSE'] for m in models]
        bars = ax2.bar(models, gen_rmse_values, alpha=0.8, color=colors, edgecolor='black', linewidth=1.5)
        ax2.set_title('å‘ç”µæ—¶æ®µRMSEå¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
        ax2.set_ylabel('RMSE (MW)', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')
        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=12)
        
        for bar, value in zip(bars, gen_rmse_values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        # 3. ç›¸å…³ç³»æ•°å¯¹æ¯”
        ax3 = axes[0, 2]
        corr_values = [self.metrics[m]['Generation_Correlation'] for m in models]
        bars = ax3.bar(models, corr_values, alpha=0.8, color=colors, edgecolor='black', linewidth=1.5)
        ax3.set_title('å‘ç”µæ—¶æ®µç›¸å…³ç³»æ•°å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
        ax3.set_ylabel('ç›¸å…³ç³»æ•°', fontsize=14, fontweight='bold')
        ax3.grid(True, alpha=0.3, axis='y')
        ax3.set_ylim(0, 1.0)
        plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=12)
        
        for bar, value in zip(bars, corr_values):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        # 4. é™å°ºåº¦æ”¹å–„æ•ˆæžœ
        ax4 = axes[1, 0]
        if 'original_nwp' in self.metrics:
            baseline_r2 = self.metrics['original_nwp']['Generation_R2']
            baseline_rmse = self.metrics['original_nwp']['Generation_RMSE']
            
            improvements_r2 = [self.metrics[m]['Generation_R2'] - baseline_r2 for m in models if m != 'original_nwp']
            improvements_rmse = [(baseline_rmse - self.metrics[m]['Generation_RMSE']) / baseline_rmse * 100 for m in models if m != 'original_nwp']
            improvement_models = [m for m in models if m != 'original_nwp']
            improvement_colors = [colors[i] for i, m in enumerate(models) if m != 'original_nwp']
            
            x = np.arange(len(improvement_models))
            width = 0.35
            
            bars1 = ax4.bar(x - width/2, improvements_r2, width, label='R^2æ”¹å–„', alpha=0.8, color='skyblue', edgecolor='black')
            ax4_twin = ax4.twinx()
            bars2 = ax4_twin.bar(x + width/2, improvements_rmse, width, label='RMSEæ”¹å–„(%)', alpha=0.8, color='lightcoral', edgecolor='black')
            
            ax4.set_xlabel('æ¨¡åž‹', fontsize=14, fontweight='bold')
            ax4.set_ylabel('R^2æ”¹å–„', fontsize=14, fontweight='bold', color='blue')
            ax4_twin.set_ylabel('RMSEæ”¹å–„ (%)', fontsize=14, fontweight='bold', color='red')
            ax4.set_title('ç›¸å¯¹åŽŸå§‹NWPçš„æ”¹å–„æ•ˆæžœ', fontsize=16, fontweight='bold', pad=20)
            ax4.set_xticks(x)
            ax4.set_xticklabels(improvement_models, rotation=45, ha='right', fontsize=12)
            ax4.grid(True, alpha=0.3)
            ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars1, improvements_r2):
                ax4.text(bar.get_x() + bar.get_width()/2, 
                        bar.get_height() + (0.002 if value >= 0 else -0.005),
                        f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top', 
                        fontsize=11, fontweight='bold')
            
            for bar, value in zip(bars2, improvements_rmse):
                ax4_twin.text(bar.get_x() + bar.get_width()/2, 
                             bar.get_height() + (0.5 if value >= 0 else -1.0),
                             f'{value:.1f}%', ha='center', va='bottom' if value >= 0 else 'top', 
                             fontsize=11, fontweight='bold')
            
            # å›¾ä¾‹
            lines1, labels1 = ax4.get_legend_handles_labels()
            lines2, labels2 = ax4_twin.get_legend_handles_labels()
            ax4.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
        
        # 5. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        ax5 = axes[1, 1]
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        categories = ['R^2', 'RMSE\n(åå‘)', 'ç›¸å…³ç³»æ•°', 'MAE\n(åå‘)']
        N = len(categories)
        
        # è®¡ç®—è§’åº¦
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]  # é—­åˆ
        
        ax5 = plt.subplot(2, 3, 5, projection='polar')
        
        for i, model in enumerate(models[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªæ¨¡åž‹
            metrics = self.metrics[model]
            values = [
                metrics['Generation_R2'],
                1 - metrics['Generation_RMSE'] / max(gen_rmse_values),  # åå‘RMSE
                metrics['Generation_Correlation'],
                1 - metrics['Generation_MAE'] / max([self.metrics[m]['Generation_MAE'] for m in models])  # åå‘MAE
            ]
            values += values[:1]  # é—­åˆ
            
            ax5.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])
            ax5.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax5.set_xticks(angles[:-1])
        ax5.set_xticklabels(categories, fontsize=12)
        ax5.set_ylim(0, 1)
        ax5.set_title('ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', fontsize=16, fontweight='bold', pad=30)
        ax5.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax5.grid(True)
        
        # 6. æ ·æœ¬æ•°é‡å’Œæ•ˆæžœæ€»ç»“
        ax6 = axes[1, 2]
        sample_counts = [self.metrics[m]['Generation_Sample_Count'] for m in models]
        
        # åˆ›å»ºè¡¨æ ¼æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        table_data = []
        for model in models:
            metrics = self.metrics[model]
            table_data.append([
                model,
                f"{metrics['Generation_R2']:.3f}",
                f"{metrics['Generation_RMSE']:.3f}",
                f"{metrics['Generation_Sample_Count']}"
            ])
        
        table = ax6.table(cellText=table_data,
                         colLabels=['æ¨¡åž‹', 'R^2', 'RMSE', 'æ ·æœ¬æ•°'],
                         cellLoc='center',
                         loc='center',
                         bbox=[0, 0, 1, 1])
        
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1, 2)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for i in range(len(models) + 1):
            for j in range(4):
                cell = table[(i, j)]
                if i == 0:  # è¡¨å¤´
                    cell.set_facecolor('#4CAF50')
                    cell.set_text_props(weight='bold', color='white')
                else:
                    cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')
                cell.set_edgecolor('black')
                cell.set_linewidth(1)
        
        ax6.set_title('æ¨¡åž‹æ€§èƒ½æ±‡æ€»è¡¨', fontsize=16, fontweight='bold', pad=20)
        ax6.axis('off')
        
        # ç¾ŽåŒ–æ•´ä½“å›¾è¡¨
        for ax in axes.flat:
            if ax != ax5 and ax != ax6:  # æŽ’é™¤é›·è¾¾å›¾å’Œè¡¨æ ¼
                ax.spines['top'].set_visible(False)
                ax.spines['right'].set_visible(False)
                ax.spines['left'].set_linewidth(1.5)
                ax.spines['bottom'].set_linewidth(1.5)
                ax.tick_params(axis='both', which='major', labelsize=12)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.93)
        # ä¿å­˜å¤šä¸ªç‰ˆæœ¬
        plt.savefig(folder / f'{self.station_id}_ç»¼åˆæ€§èƒ½å¯¹æ¯”.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.savefig(folder / f'{self.station_id}_optimized_comprehensive_performance.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
    
    def plot_prediction_analysis(self, folder):
        """ç»˜åˆ¶é¢„æµ‹æ•ˆæžœåˆ†æž"""
        print("ðŸ“Š ç»˜åˆ¶ä¼˜åŒ–çš„é¢„æµ‹æ•ˆæžœåˆ†æž...")
        
        actual = self.test_data['power'].values
        generation_mask = self.test_data['is_generation_time'] == 1
        
        if generation_mask.sum() == 0:
            print("âš ï¸ æ²¡æœ‰å‘ç”µæ—¶æ®µæ•°æ®ï¼Œè·³è¿‡é¢„æµ‹åˆ†æžå›¾")
            return
        
        # è®¡ç®—éœ€è¦çš„å­å›¾æ•°é‡
        n_models = len(self.predictions)
        n_cols = min(3, n_models)
        n_rows = (n_models + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 6*n_rows))
        if n_models == 1:
            axes = [axes]
        elif n_rows == 1 and n_cols > 1:
            axes = axes
        elif n_rows > 1:
            axes = axes.flatten()
        else:
            axes = [axes]
        
        fig.suptitle(f'{self.station_id} é¢„æµ‹æ•ˆæžœåˆ†æžï¼ˆå‘ç”µæ—¶æ®µï¼‰', fontsize=18, fontweight='bold', y=0.98)
        
        # åªæ˜¾ç¤ºå‘ç”µæ—¶æ®µæ•°æ®
        actual_gen = actual[generation_mask]
        
        for i, (model_name, predictions) in enumerate(self.predictions.items()):
            if i >= len(axes):
                break
            
            ax = axes[i]
            pred_gen = predictions[generation_mask]
            
            # åˆ›å»ºå¯†åº¦æ•£ç‚¹å›¾
            from matplotlib.colors import LinearSegmentedColormap
            
            # æ•£ç‚¹å›¾ï¼Œæ ¹æ®å¯†åº¦ç€è‰²
            scatter = ax.scatter(actual_gen, pred_gen, alpha=0.7, s=25, 
                               c=range(len(actual_gen)), cmap='plasma', edgecolors='none')
            
            # ç†æƒ³çº¿
            max_val = max(actual_gen.max(), pred_gen.max())
            min_val = min(actual_gen.min(), pred_gen.min())
            ax.plot([min_val, max_val], [min_val, max_val], 'r-', alpha=0.8, linewidth=3, label='ç†æƒ³é¢„æµ‹')
            
            # æ·»åŠ æ‹Ÿåˆçº¿
            z = np.polyfit(actual_gen, pred_gen, 1)
            p = np.poly1d(z)
            ax.plot([min_val, max_val], p([min_val, max_val]), 'g--', alpha=0.8, linewidth=2, label='æ‹Ÿåˆçº¿')
            
            # ç»Ÿè®¡ä¿¡æ¯
            r2 = self.metrics[model_name]['Generation_R2']
            rmse = self.metrics[model_name]['Generation_RMSE']
            corr = self.metrics[model_name]['Generation_Correlation']
            mae = self.metrics[model_name]['Generation_MAE']
            
            # è®¡ç®—é¢å¤–ç»Ÿè®¡ä¿¡æ¯
            mape = np.mean(np.abs((actual_gen - pred_gen) / (actual_gen + 1e-6))) * 100
            bias = np.mean(pred_gen - actual_gen)
            
            # æ·»åŠ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
            stats_text = (f'R^2 = {r2:.3f}\n'
                         f'RMSE = {rmse:.3f} MW\n'
                         f'MAE = {mae:.3f} MW\n'
                         f'ç›¸å…³ç³»æ•° = {corr:.3f}\n'
                         f'MAPE = {mape:.1f}%\n'
                         f'åå·® = {bias:.3f} MW\n'
                         f'æ ·æœ¬æ•° = {len(actual_gen)}')
            
            ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, va='top', ha='left',
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor='gray'),
                   fontsize=11, family='monospace')
            
            ax.set_xlabel('å®žé™…åŠŸçŽ‡ (MW)', fontsize=14, fontweight='bold')
            ax.set_ylabel('é¢„æµ‹åŠŸçŽ‡ (MW)', fontsize=14, fontweight='bold')
            ax.set_title(f'{model_name}', fontsize=16, fontweight='bold', pad=20)
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.legend(loc='lower right', fontsize=12)
            
            # è®¾ç½®ç›¸ç­‰çš„åæ ‡è½´èŒƒå›´
            margin = (max_val - min_val) * 0.05
            ax.set_xlim(min_val - margin, max_val + margin)
            ax.set_ylim(min_val - margin, max_val + margin)
            ax.set_aspect('equal')
            
            # ç¾ŽåŒ–åæ ‡è½´
            ax.tick_params(axis='both', which='major', labelsize=12)
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['left'].set_linewidth(1.5)
            ax.spines['bottom'].set_linewidth(1.5)
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(self.predictions), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.93)
        # ä¿å­˜å¤šä¸ªç‰ˆæœ¬
        plt.savefig(folder / f'{self.station_id}_é¢„æµ‹æ•ˆæžœåˆ†æž.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.savefig(folder / f'{self.station_id}_optimized_prediction_analysis.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
    
    def plot_detailed_time_series(self, folder):
        """ç»˜åˆ¶è¯¦ç»†æ—¶é—´åºåˆ—åˆ†æž"""
        print("ðŸ“Š ç»˜åˆ¶ä¼˜åŒ–çš„æ—¶é—´åºåˆ—åˆ†æž...")
        
        fig, axes = plt.subplots(4, 1, figsize=(20, 20))
        fig.suptitle(f'{self.station_id} ä¼˜åŒ–æ—¶é—´åºåˆ—åˆ†æž', fontsize=16, fontweight='bold')
        
        # é€‰æ‹©ä¸€ä¸ªæœ‰ä»£è¡¨æ€§çš„æ—¶é—´æ®µè¿›è¡Œè¯¦ç»†å±•ç¤ºï¼ˆæ¯”å¦‚è¿žç»­7å¤©çš„æ•°æ®ï¼‰
        test_data_sorted = self.test_data.sort_values('date_time')
        
        # æ‰¾åˆ°ä¸€ä¸ªåŒ…å«è¾ƒå¤šå‘ç”µæ•°æ®çš„è¿žç»­æ—¶é—´æ®µ
        generation_data = test_data_sorted[test_data_sorted['is_generation_time'] == 1]
        if len(generation_data) > 0:
            # é€‰æ‹©å‘ç”µæ•°æ®è¾ƒå¤šçš„ä¸€å‘¨
            start_date = generation_data['date_time'].iloc[len(generation_data)//3]
            end_date = start_date + pd.Timedelta(days=7)
            
            # ç¡®ä¿ä¸è¶…å‡ºæ•°æ®èŒƒå›´
            if end_date > test_data_sorted['date_time'].max():
                end_date = test_data_sorted['date_time'].max()
                start_date = end_date - pd.Timedelta(days=7)
        else:
            # å¦‚æžœæ²¡æœ‰å‘ç”µæ•°æ®ï¼Œé€‰æ‹©ä¸­é—´çš„ä¸€å‘¨
            start_date = test_data_sorted['date_time'].iloc[len(test_data_sorted)//2]
            end_date = start_date + pd.Timedelta(days=7)
        
        # ç­›é€‰æ—¶é—´æ®µæ•°æ®
        mask = (test_data_sorted['date_time'] >= start_date) & (test_data_sorted['date_time'] <= end_date)
        plot_data = test_data_sorted[mask].copy()
        
        if len(plot_data) == 0:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ—¶é—´æ®µæ•°æ®ï¼Œä½¿ç”¨å…¨éƒ¨æµ‹è¯•æ•°æ®çš„é‡‡æ ·")
            # å¦‚æžœç­›é€‰åŽæ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨é‡‡æ ·
            plot_data = test_data_sorted.iloc[::max(1, len(test_data_sorted)//500)].copy()
        
        time_range = plot_data['date_time']
        actual = plot_data['power']
        generation_mask = plot_data['is_generation_time'] == 1
        
        print(f"  å¯è§†åŒ–æ—¶é—´æ®µ: {time_range.min()} åˆ° {time_range.max()}")
        print(f"  æ•°æ®ç‚¹æ•°: {len(plot_data)}")
        print(f"  å‘ç”µæ—¶æ®µæ¯”ä¾‹: {generation_mask.sum() / len(plot_data) * 100:.1f}%")
        
        # èŽ·å–å¯¹åº”çš„é¢„æµ‹æ•°æ®
        plot_predictions = {}
        for model_name, full_predictions in self.predictions.items():
            # æ‰¾åˆ°å¯¹åº”çš„é¢„æµ‹å€¼ - ä¿®å¤ç´¢å¼•é—®é¢˜
            # ä½¿ç”¨å¸ƒå°”ç´¢å¼•è€Œä¸æ˜¯ä½ç½®ç´¢å¼•
            test_data_mask = self.test_data['date_time'].isin(plot_data['date_time'])
            plot_predictions[model_name] = full_predictions[test_data_mask]
        
        # ç¬¬ä¸€ä¸ªå›¾ï¼šå…¨æ—¶æ®µå¯¹æ¯”ï¼ˆåŒ…å«éžå‘ç”µæ—¶æ®µï¼‰
        ax1 = axes[0]
        ax1.plot(time_range, actual, label='å®žé™…åŠŸçŽ‡', linewidth=2, alpha=0.9, color='black', marker='o', markersize=2)
        
        # é€‰æ‹©æœ€ä½³æ¨¡åž‹
        best_model = max(self.metrics.keys(), key=lambda x: self.metrics[x]['Generation_R2'])
        ax1.plot(time_range, plot_predictions[best_model], label=f'æœ€ä½³æ¨¡åž‹({best_model})', 
                linewidth=2, alpha=0.8, color='red', marker='s', markersize=2)
        
        if 'original_nwp' in plot_predictions:
            ax1.plot(time_range, plot_predictions['original_nwp'], label='åŽŸå§‹NWP', 
                    linewidth=2, alpha=0.8, color='blue', marker='^', markersize=2)
        
        if 'downscaled_nwp' in plot_predictions:
            ax1.plot(time_range, plot_predictions['downscaled_nwp'], label='é™å°ºåº¦NWP', 
                    linewidth=2, alpha=0.8, color='green', marker='d', markersize=2)
        
        # æ ‡è®°å‘ç”µæ—¶æ®µèƒŒæ™¯
        ax1.fill_between(time_range, 0, actual.max() * 1.1, 
                        where=generation_mask, alpha=0.2, color='yellow', label='å‘ç”µæ—¶æ®µ')
        
        ax1.set_title(f'å…¨æ—¶æ®µé¢„æµ‹å¯¹æ¯” ({start_date.strftime("%m-%d")} åˆ° {end_date.strftime("%m-%d")})', fontsize=14)
        ax1.set_ylabel('åŠŸçŽ‡ (MW)', fontsize=12)
        ax1.legend(loc='upper right')
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(bottom=0)
        
        # ç¬¬äºŒä¸ªå›¾ï¼šåªæ˜¾ç¤ºå‘ç”µæ—¶æ®µ
        ax2 = axes[1]
        if generation_mask.sum() > 0:
            generation_time = time_range[generation_mask]
            generation_actual = actual[generation_mask]
            
            ax2.plot(generation_time, generation_actual, label='å®žé™…åŠŸçŽ‡', 
                    linewidth=3, alpha=0.9, color='black', marker='o', markersize=4)
            
            if best_model in plot_predictions:
                generation_pred = plot_predictions[best_model][generation_mask]
                ax2.plot(generation_time, generation_pred, label=f'æœ€ä½³æ¨¡åž‹({best_model})', 
                        linewidth=3, alpha=0.8, color='red', marker='s', markersize=4)
            
            if 'downscaled_nwp' in plot_predictions:
                downscaled_pred = plot_predictions['downscaled_nwp'][generation_mask]
                ax2.plot(generation_time, downscaled_pred, label='é™å°ºåº¦NWP', 
                        linewidth=3, alpha=0.8, color='green', marker='d', markersize=4)
            
            ax2.set_title('å‘ç”µæ—¶æ®µè¯¦ç»†å¯¹æ¯”', fontsize=14)
            ax2.set_ylabel('åŠŸçŽ‡ (MW)', fontsize=12)
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            ax2.set_ylim(bottom=0)
        else:
            ax2.text(0.5, 0.5, 'è¯¥æ—¶é—´æ®µæ— å‘ç”µæ•°æ®', ha='center', va='center', 
                    transform=ax2.transAxes, fontsize=14)
            ax2.set_title('å‘ç”µæ—¶æ®µè¯¦ç»†å¯¹æ¯”ï¼ˆæ— æ•°æ®ï¼‰', fontsize=14)
        
        # ç¬¬ä¸‰ä¸ªå›¾ï¼šé¢„æµ‹è¯¯å·®åˆ†æž
        ax3 = axes[2]
        if best_model in plot_predictions:
            errors = plot_predictions[best_model] - actual
            ax3.plot(time_range, errors, label=f'{best_model}è¯¯å·®', 
                    linewidth=2, alpha=0.8, color='red', marker='o', markersize=2)
        
        if 'downscaled_nwp' in plot_predictions:
            downscaled_errors = plot_predictions['downscaled_nwp'] - actual
            ax3.plot(time_range, downscaled_errors, label='é™å°ºåº¦NWPè¯¯å·®', 
                    linewidth=2, alpha=0.8, color='green', marker='s', markersize=2)
        
        # æ·»åŠ è¯¯å·®ç»Ÿè®¡ä¿¡æ¯
        if best_model in plot_predictions:
            rmse = np.sqrt(np.mean(errors**2))
            mae = np.mean(np.abs(errors))
            ax3.text(0.02, 0.98, f'RMSE: {rmse:.3f} MW\nMAE: {mae:.3f} MW', 
                    transform=ax3.transAxes, va='top', ha='left',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        ax3.set_title('é¢„æµ‹è¯¯å·®å¯¹æ¯”', fontsize=14)
        ax3.set_ylabel('é¢„æµ‹è¯¯å·® (MW)', fontsize=12)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        
        # ç¬¬å››ä¸ªå›¾ï¼šæ¨¡åž‹æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
        ax4 = axes[3]
        models = list(self.metrics.keys())
        r2_values = [self.metrics[m]['Generation_R2'] for m in models]
        rmse_values = [self.metrics[m]['Generation_RMSE'] for m in models]
        
        x = np.arange(len(models))
        width = 0.35
        
        # åˆ›å»ºåŒyè½´
        ax4_twin = ax4.twinx()
        
        bars1 = ax4.bar(x - width/2, r2_values, width, label='R^2', alpha=0.8, color='skyblue')
        bars2 = ax4_twin.bar(x + width/2, rmse_values, width, label='RMSE', alpha=0.8, color='lightcoral')
        
        ax4.set_xlabel('æ¨¡åž‹', fontsize=12)
        ax4.set_ylabel('R^2 å€¼', fontsize=12, color='blue')
        ax4_twin.set_ylabel('RMSE (MW)', fontsize=12, color='red')
        ax4.set_title('æ¨¡åž‹æ€§èƒ½ç»¼åˆå¯¹æ¯”', fontsize=14)
        ax4.set_xticks(x)
        ax4.set_xticklabels(models, rotation=45, ha='right')
        ax4.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars1, r2_values):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        for bar, value in zip(bars2, rmse_values):
            ax4_twin.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                         f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        # å›¾ä¾‹
        lines1, labels1 = ax4.get_legend_handles_labels()
        lines2, labels2 = ax4_twin.get_legend_handles_labels()
        ax4.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
        
        # æ ¼å¼åŒ–xè½´æ—¶é—´æ ‡ç­¾ï¼ˆå‰ä¸‰ä¸ªå›¾ï¼‰
        for ax in axes[:3]:
            ax.xaxis.set_major_formatter(DateFormatter('%m-%d %H:%M'))
            ax.xaxis.set_major_locator(mdates.HourLocator(interval=max(1, len(plot_data)//10)))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        # ä¿å­˜å¤šä¸ªç‰ˆæœ¬
        plt.savefig(folder / f'{self.station_id}_æ—¶é—´åºåˆ—åˆ†æž.png', 
                   dpi=300, bbox_inches='tight')
        plt.savefig(folder / f'{self.station_id}_optimized_time_series.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # é¢å¤–ç”Ÿæˆä¸€ä¸ªå‘ç”µæ—¶æ®µçš„æ•£ç‚¹å¯¹æ¯”å›¾
        self.plot_generation_scatter_comparison(folder)
    
    def plot_generation_scatter_comparison(self, folder):
        """ç»˜åˆ¶å‘ç”µæ—¶æ®µçš„æ•£ç‚¹å¯¹æ¯”å›¾"""
        print("ðŸ“Š ç»˜åˆ¶å‘ç”µæ—¶æ®µæ•£ç‚¹å¯¹æ¯”å›¾...")
        
        actual = self.test_data['power'].values
        generation_mask = self.test_data['is_generation_time'] == 1
        
        if generation_mask.sum() == 0:
            print("âš ï¸ æ²¡æœ‰å‘ç”µæ—¶æ®µæ•°æ®ï¼Œè·³è¿‡æ•£ç‚¹å›¾")
            return
        
        actual_gen = actual[generation_mask]
        
        # è®¡ç®—éœ€è¦çš„å­å›¾æ•°é‡
        n_models = len(self.predictions)
        n_cols = min(3, n_models)
        n_rows = (n_models + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))
        if n_models == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = [axes]
        else:
            axes = axes.flatten()
        
        fig.suptitle(f'{self.station_id} å‘ç”µæ—¶æ®µé¢„æµ‹ç²¾åº¦å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        for i, (model_name, predictions) in enumerate(self.predictions.items()):
            if i >= len(axes):
                break
                
            ax = axes[i]
            pred_gen = predictions[generation_mask]
            
            # æ•£ç‚¹å›¾
            scatter = ax.scatter(actual_gen, pred_gen, alpha=0.6, s=30, c=pred_gen, cmap='viridis')
            
            # ç†æƒ³çº¿
            max_val = max(actual_gen.max(), pred_gen.max())
            min_val = min(actual_gen.min(), pred_gen.min())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2, label='ç†æƒ³é¢„æµ‹')
            
            # ç»Ÿè®¡ä¿¡æ¯
            r2 = self.metrics[model_name]['Generation_R2']
            rmse = self.metrics[model_name]['Generation_RMSE']
            corr = self.metrics[model_name]['Generation_Correlation']
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
            stats_text = f'R^2 = {r2:.3f}\nRMSE = {rmse:.3f} MW\nç›¸å…³ç³»æ•° = {corr:.3f}\næ ·æœ¬æ•° = {len(actual_gen)}'
            ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, va='top', ha='left',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=10)
            
            ax.set_xlabel('å®žé™…åŠŸçŽ‡ (MW)', fontsize=12)
            ax.set_ylabel('é¢„æµ‹åŠŸçŽ‡ (MW)', fontsize=12)
            ax.set_title(f'{model_name}', fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend()
            
            # è®¾ç½®ç›¸ç­‰çš„åæ ‡è½´èŒƒå›´
            ax.set_xlim(min_val, max_val)
            ax.set_ylim(min_val, max_val)
            ax.set_aspect('equal')
            
            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(scatter, ax=ax, label='é¢„æµ‹åŠŸçŽ‡ (MW)')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(self.predictions), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        # ä¿å­˜å¤šä¸ªç‰ˆæœ¬
        plt.savefig(folder / f'{self.station_id}_å‘ç”µæ—¶æ®µæ•£ç‚¹å¯¹æ¯”.png', 
                   dpi=300, bbox_inches='tight')
        plt.savefig(folder / f'{self.station_id}_generation_scatter_comparison.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_downscaling_analysis(self, folder):
        """ç»˜åˆ¶é™å°ºåº¦æ•ˆæžœåˆ†æž"""
        if 'original_nwp' not in self.predictions or 'downscaled_nwp' not in self.predictions:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{self.station_id} ç©ºé—´é™å°ºåº¦æŠ€æœ¯æ•ˆæžœåˆ†æž', fontsize=16, fontweight='bold')
        
        actual = self.test_data['power'].values
        original_pred = self.predictions['original_nwp']
        downscaled_pred = self.predictions['downscaled_nwp']
        generation_mask = self.test_data['is_generation_time'] == 1
        
        # å‘ç”µæ—¶æ®µæ•°æ®
        actual_gen = actual[generation_mask]
        original_gen = original_pred[generation_mask]
        downscaled_gen = downscaled_pred[generation_mask]
        
        # 1. åŽŸå§‹NWP vs å®žé™…
        ax1 = axes[0, 0]
        ax1.scatter(actual_gen, original_gen, alpha=0.6, s=20, color='blue')
        max_val = max(actual_gen.max(), original_gen.max())
        ax1.plot([0, max_val], [0, max_val], 'r--', alpha=0.8)
        r2_orig = self.metrics['original_nwp']['Generation_R2']
        ax1.set_title(f'åŽŸå§‹NWPé¢„æµ‹\nR^2 = {r2_orig:.3f}')
        ax1.set_xlabel('å®žé™…åŠŸçŽ‡ (MW)')
        ax1.set_ylabel('é¢„æµ‹åŠŸçŽ‡ (MW)')
        ax1.grid(True, alpha=0.3)
        
        # 2. é™å°ºåº¦NWP vs å®žé™…
        ax2 = axes[0, 1]
        ax2.scatter(actual_gen, downscaled_gen, alpha=0.6, s=20, color='green')
        ax2.plot([0, max_val], [0, max_val], 'r--', alpha=0.8)
        r2_down = self.metrics['downscaled_nwp']['Generation_R2']
        ax2.set_title(f'é™å°ºåº¦NWPé¢„æµ‹\nR^2 = {r2_down:.3f}')
        ax2.set_xlabel('å®žé™…åŠŸçŽ‡ (MW)')
        ax2.set_ylabel('é¢„æµ‹åŠŸçŽ‡ (MW)')
        ax2.grid(True, alpha=0.3)
        
        # 3. è¯¯å·®å¯¹æ¯”
        ax3 = axes[1, 0]
        original_errors = original_gen - actual_gen
        downscaled_errors = downscaled_gen - actual_gen
        
        ax3.hist(original_errors, bins=30, alpha=0.7, label='åŽŸå§‹NWPè¯¯å·®', color='blue')
        ax3.hist(downscaled_errors, bins=30, alpha=0.7, label='é™å°ºåº¦NWPè¯¯å·®', color='green')
        ax3.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒå¯¹æ¯”')
        ax3.set_xlabel('é¢„æµ‹è¯¯å·® (MW)')
        ax3.set_ylabel('é¢‘æ¬¡')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. æ”¹å–„æ•ˆæžœæ€»ç»“
        ax4 = axes[1, 1]
        metrics_comparison = {
            'R^2': [r2_orig, r2_down],
            'RMSE': [self.metrics['original_nwp']['Generation_RMSE'], 
                    self.metrics['downscaled_nwp']['Generation_RMSE']],
            'MAE': [self.metrics['original_nwp']['Generation_MAE'], 
                   self.metrics['downscaled_nwp']['Generation_MAE']],
            'Correlation': [self.metrics['original_nwp']['Generation_Correlation'], 
                           self.metrics['downscaled_nwp']['Generation_Correlation']]
        }
        
        x = np.arange(len(metrics_comparison))
        width = 0.35
        
        for i, (metric, values) in enumerate(metrics_comparison.items()):
            ax4.bar(i - width/2, values[0], width, label='åŽŸå§‹NWP', alpha=0.7, color='blue')
            ax4.bar(i + width/2, values[1], width, label='é™å°ºåº¦NWP', alpha=0.7, color='green')
        
        ax4.set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
        ax4.set_ylabel('æŒ‡æ ‡å€¼')
        ax4.set_xticks(x)
        ax4.set_xticklabels(metrics_comparison.keys())
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        # ä¿å­˜å¤šä¸ªç‰ˆæœ¬
        plt.savefig(folder / f'{self.station_id}_é™å°ºåº¦æ•ˆæžœåˆ†æž.png', 
                   dpi=300, bbox_inches='tight')
        plt.savefig(folder / f'{self.station_id}_final_downscaling_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_comprehensive_results(self):
        """ä¿å­˜ç»¼åˆç»“æžœ"""
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜é¢„æµ‹ç»“æžœ
        results_df = pd.DataFrame({
            'date_time': self.test_data['date_time'],
            'actual_power': self.test_data['power'],
            'is_generation_time': self.test_data['is_generation_time'],
            'is_effective_generation': self.test_data['is_effective_generation']
        })
        
        for model_name, predictions in self.predictions.items():
            results_df[f'{model_name}_prediction'] = predictions
            results_df[f'{model_name}_error'] = predictions - self.test_data['power']
        
        results_df.to_csv(results_dir / f'{self.station_id}_final_optimized_results.csv', index=False)
        
        # ä¿å­˜è¯„ä»·æŒ‡æ ‡
        metrics_df = pd.DataFrame(self.metrics).T
        metrics_df.to_csv(results_dir / f'{self.station_id}_final_optimized_metrics.csv')
        
        # ä¿å­˜åˆ†æžæ€»ç»“
        self.save_analysis_summary()
        
        print(f"âœ… ç»¼åˆç»“æžœå·²ä¿å­˜åˆ° results/ ç›®å½•")
    
    def save_analysis_summary(self):
        """ä¿å­˜åˆ†æžæ€»ç»“æŠ¥å‘Š"""
        summary_path = Path("results") / f"{self.station_id}_final_analysis_summary.md"
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"# {self.station_id} æœ€ç»ˆä¼˜åŒ–ç©ºé—´é™å°ºåº¦åˆ†æžæŠ¥å‘Š\n\n")
            f.write(f"## åˆ†æžæ¦‚è¿°\n")
            f.write(f"- åˆ†æžæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"- æ•°æ®å®¹é‡: {self.capacity:.3f} MW\n")
            f.write(f"- å‘ç”µé˜ˆå€¼: {self.generation_threshold} MW\n\n")
            
            f.write(f"## æ¨¡åž‹æ€§èƒ½å¯¹æ¯”\n\n")
            f.write("| æ¨¡åž‹ | å…¨æ•°æ®R^2 | å‘ç”µæ—¶æ®µR^2 | å‘ç”µæ—¶æ®µRMSE | ç›¸å…³ç³»æ•° | å‘ç”µæ ·æœ¬æ•° |\n")
            f.write("|------|----------|------------|--------------|----------|------------|\n")
            
            for model_name, metrics in self.metrics.items():
                f.write(f"| {model_name} | {metrics['All_R2']:.4f} | {metrics['Generation_R2']:.4f} | "
                       f"{metrics['Generation_RMSE']:.4f} | {metrics['Generation_Correlation']:.4f} | "
                       f"{metrics['Generation_Sample_Count']} |\n")
            
            # é™å°ºåº¦æœ‰æ•ˆæ€§åˆ†æž
            if 'original_nwp' in self.metrics and 'downscaled_nwp' in self.metrics:
                orig = self.metrics['original_nwp']
                down = self.metrics['downscaled_nwp']
                
                r2_improvement = down['Generation_R2'] - orig['Generation_R2']
                rmse_improvement = (orig['Generation_RMSE'] - down['Generation_RMSE']) / orig['Generation_RMSE'] * 100
                
                f.write(f"\n## ç©ºé—´é™å°ºåº¦æŠ€æœ¯æœ‰æ•ˆæ€§\n\n")
                f.write(f"- å‘ç”µæ—¶æ®µR^2æ”¹å–„: {r2_improvement:.4f}\n")
                f.write(f"- å‘ç”µæ—¶æ®µRMSEæ”¹å–„: {rmse_improvement:.2f}%\n")
                
                if r2_improvement > 0.05 and rmse_improvement > 5:
                    effectiveness = "æ˜¾è‘—æœ‰æ•ˆ"
                elif r2_improvement > 0.02 and rmse_improvement > 2:
                    effectiveness = "æœ‰æ•ˆ"
                elif r2_improvement > 0:
                    effectiveness = "è½»å¾®æœ‰æ•ˆ"
                else:
                    effectiveness = "æ— æ•ˆæˆ–è´Ÿé¢å½±å“"
                
                f.write(f"- æœ‰æ•ˆæ€§ç­‰çº§: **{effectiveness}**\n\n")
            
            f.write(f"## ä¸»è¦å‘çŽ°\n\n")
            f.write(f"1. æ•°æ®ç‰¹ç‚¹ï¼šstation00å…·æœ‰æ˜Žæ˜¾çš„é—´æ­‡æ€§å‘ç”µç‰¹å¾\n")
            f.write(f"2. æ¨¡åž‹è¡¨çŽ°ï¼šä¸“æ³¨äºŽå‘ç”µæ—¶æ®µçš„æ¨¡åž‹è¡¨çŽ°æ›´å¥½\n")
            f.write(f"3. é™å°ºåº¦æ•ˆæžœï¼šç©ºé—´é™å°ºåº¦æŠ€æœ¯å¯¹è¯¥ç«™ç‚¹çš„æ•ˆæžœæœ‰é™\n")
            f.write(f"4. å»ºè®®ï¼šéœ€è¦æ›´ç²¾ç»†çš„å±€åœ°æ°”è±¡ç‰¹å¾å’Œæ›´é•¿çš„åŽ†å²æ•°æ®\n")
    
    def run_final_analysis(self):
        """è¿è¡Œæœ€ç»ˆä¼˜åŒ–åˆ†æž"""
        print(f"ðŸŽ¯ å¼€å§‹ {self.station_id} æœ€ç»ˆä¼˜åŒ–ç©ºé—´é™å°ºåº¦åˆ†æž...")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. æ•°æ®åŠ è½½å’Œæ·±åº¦åˆ†æž
        df = self.load_and_analyze_data()
        
        # 2. é«˜çº§ç‰¹å¾å·¥ç¨‹
        data = self.create_advanced_features(df)
        
        # 3. æ™ºèƒ½æ•°æ®åˆ†å‰²
        self.train_data, self.test_data = self.smart_data_split(data)
        
        # 4. è®­ç»ƒä¼˜åŒ–æ¨¡åž‹
        self.train_optimized_models(self.train_data)
        
        # 5. é¢„æµ‹å’ŒåŽå¤„ç†
        self.predictions = self.predict_with_postprocessing(self.test_data)
        
        # 6. é™å°ºåº¦æœ‰æ•ˆæ€§åˆ†æž
        effectiveness = self.analyze_downscaling_effectiveness()
        
        # 7. ç»¼åˆå¯è§†åŒ–
        self.create_comprehensive_visualizations()
        
        # 8. ä¿å­˜ç»¼åˆç»“æžœ
        self.save_comprehensive_results()
        
        return effectiveness

def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ å¼€å§‹station00æœ€ç»ˆä¼˜åŒ–ç©ºé—´é™å°ºåº¦åˆ†æž")
    
    try:
        analyzer = FinalOptimizedStation00Analysis()
        effectiveness = analyzer.run_final_analysis()
        
        print(f"\n{'='*100}")
        print("ðŸŽ‰ station00æœ€ç»ˆä¼˜åŒ–åˆ†æžå®Œæˆï¼")
        print(f"{'='*100}")
        print(f"ç©ºé—´é™å°ºåº¦æŠ€æœ¯æœ‰æ•ˆæ€§: {effectiveness}")
        print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        print(f"âŒ åˆ†æžè¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 