# èå…¥NWPä¿¡æ¯çš„å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹æ¨¡å‹ - é—®é¢˜3
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
import joblib
from pathlib import Path
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates

warnings.filterwarnings('ignore')

# ç®€åŒ–çš„ä¸­æ–‡å­—ä½“è®¾ç½®
import matplotlib as mpl
font_path = 'C:/Windows/Fonts/simhei.ttf'
mpl.font_manager.fontManager.addfont(font_path)  
mpl.rc('font', family='simhei')
plt.rcParams['axes.unicode_minus'] = False

sns.set_palette("husl")
plt.ioff()

class NWPPowerPrediction:
    """èå…¥NWPä¿¡æ¯çš„å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, station_id: str):
        self.station_id = station_id
        self.models = {}  # å­˜å‚¨ä¸åŒæ¨¡å‹
        self.feature_names = []
        self.capacity = None
        self.train_data = None
        self.test_data = None
        self.predictions = {}  # å­˜å‚¨ä¸åŒæ¨¡å‹çš„é¢„æµ‹ç»“æœ
        self.metrics = {}  # å­˜å‚¨ä¸åŒæ¨¡å‹çš„è¯„ä»·æŒ‡æ ‡
        
    def ensure_chinese_font(self):
        """ç¡®ä¿ä¸­æ–‡å­—ä½“è®¾ç½®æ­£ç¡®åº”ç”¨"""
        mpl.rc('font', family='simhei')
        plt.rcParams['axes.unicode_minus'] = False
        
    def load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½ {self.station_id} æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(f'data/{self.station_id}.csv')
        df['date_time'] = pd.to_datetime(df['date_time'])
        df = df.sort_values('date_time').reset_index(drop=True)
        
        # ä¼°ç®—å¼€æœºå®¹é‡
        self.capacity = df['power'].max() * 1.1
        
        print(f"  æ•°æ®æ—¶é—´èŒƒå›´: {df['date_time'].min()} åˆ° {df['date_time'].max()}")
        print(f"  æ•°æ®ç‚¹æ•°: {len(df):,}")
        print(f"  å¹³å‡åŠŸç‡: {df['power'].mean():.3f} MW")
        print(f"  æœ€å¤§åŠŸç‡: {df['power'].max():.3f} MW")
        print(f"  ä¼°ç®—å¼€æœºå®¹é‡: {self.capacity:.3f} MW")
        
        # æ£€æŸ¥NWPå’ŒLMDæ•°æ®çš„å¯ç”¨æ€§
        nwp_cols = [col for col in df.columns if 'nwp_' in col]
        lmd_cols = [col for col in df.columns if 'lmd_' in col]
        
        print(f"  NWPå­—æ®µæ•°é‡: {len(nwp_cols)}")
        print(f"  LMDå­—æ®µæ•°é‡: {len(lmd_cols)}")
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        print(f"  NWPæ•°æ®ç¼ºå¤±ç‡: {df[nwp_cols].isnull().sum().sum() / (len(df) * len(nwp_cols)) * 100:.2f}%")
        print(f"  LMDæ•°æ®ç¼ºå¤±ç‡: {df[lmd_cols].isnull().sum().sum() / (len(df) * len(lmd_cols)) * 100:.2f}%")
        
        return df
    
    def create_features(self, df):
        """åˆ›å»ºç‰¹å¾å·¥ç¨‹ - åŒ…å«NWPå’ŒLMDç‰¹å¾"""
        print("ğŸ”§ åˆ›å»ºç‰¹å¾å·¥ç¨‹ï¼ˆåŒ…å«NWPä¿¡æ¯ï¼‰...")
        
        data = df.copy()
        
        # 1. æ—¶é—´ç‰¹å¾
        data['hour'] = data['date_time'].dt.hour
        data['minute'] = data['date_time'].dt.minute
        data['day_of_week'] = data['date_time'].dt.dayofweek
        data['day_of_year'] = data['date_time'].dt.dayofyear
        data['month'] = data['date_time'].dt.month
        data['time_slot'] = data['hour'] * 4 + data['minute'] // 15
        
        # 2. å‘¨æœŸæ€§ç‰¹å¾
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
        data['time_slot_sin'] = np.sin(2 * np.pi * data['time_slot'] / 96)
        data['time_slot_cos'] = np.cos(2 * np.pi * data['time_slot'] / 96)
        data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
        data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)
        
        # 3. NWPç‰¹å¾å·¥ç¨‹
        # 3.1 NWPåŸå§‹ç‰¹å¾
        nwp_features = ['nwp_globalirrad', 'nwp_directirrad', 'nwp_temperature', 
                       'nwp_humidity', 'nwp_windspeed', 'nwp_winddirection', 'nwp_pressure']
        
        # 3.2 NWPè¡ç”Ÿç‰¹å¾
        # è¾å°„ç›¸å…³ç‰¹å¾
        data['nwp_diffuse_irrad'] = data['nwp_globalirrad'] - data['nwp_directirrad']
        data['nwp_clearness_index'] = data['nwp_globalirrad'] / (data['nwp_globalirrad'].max() + 1e-6)
        data['nwp_direct_ratio'] = data['nwp_directirrad'] / (data['nwp_globalirrad'] + 1e-6)
        
        # æ¸©åº¦ç›¸å…³ç‰¹å¾
        data['nwp_temp_celsius'] = data['nwp_temperature'] - 273.15  # è½¬æ¢ä¸ºæ‘„æ°åº¦
        data['nwp_temp_optimal'] = np.abs(data['nwp_temp_celsius'] - 25)  # ä¸æœ€ä¼˜æ¸©åº¦çš„å·®å¼‚
        
        # é£é€Ÿé£å‘ç‰¹å¾
        data['nwp_wind_u'] = data['nwp_windspeed'] * np.cos(np.radians(data['nwp_winddirection']))
        data['nwp_wind_v'] = data['nwp_windspeed'] * np.sin(np.radians(data['nwp_winddirection']))
        
        # 3.3 NWPæ»åç‰¹å¾
        nwp_lag_periods = [1, 2, 4, 8, 12, 24]
        for feature in ['nwp_globalirrad', 'nwp_temperature', 'nwp_humidity']:
            for lag in nwp_lag_periods:
                data[f'{feature}_lag_{lag}'] = data[feature].shift(lag)
        
        # 3.4 NWPæ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        nwp_windows = [4, 8, 12, 24]
        for feature in ['nwp_globalirrad', 'nwp_directirrad', 'nwp_temperature']:
            for window in nwp_windows:
                data[f'{feature}_rolling_mean_{window}'] = data[feature].rolling(window=window).mean()
                data[f'{feature}_rolling_std_{window}'] = data[feature].rolling(window=window).std()
        
        # 4. LMDç‰¹å¾å·¥ç¨‹
        # 4.1 LMDåŸå§‹ç‰¹å¾
        lmd_features = ['lmd_totalirrad', 'lmd_diffuseirrad', 'lmd_temperature', 
                       'lmd_pressure', 'lmd_winddirection', 'lmd_windspeed']
        
        # 4.2 LMDè¡ç”Ÿç‰¹å¾
        data['lmd_direct_irrad'] = data['lmd_totalirrad'] - data['lmd_diffuseirrad']
        data['lmd_clearness_index'] = data['lmd_totalirrad'] / (data['lmd_totalirrad'].max() + 1e-6)
        data['lmd_diffuse_ratio'] = data['lmd_diffuseirrad'] / (data['lmd_totalirrad'] + 1e-6)
        
        # LMDé£é€Ÿé£å‘ç‰¹å¾
        data['lmd_wind_u'] = data['lmd_windspeed'] * np.cos(np.radians(data['lmd_winddirection']))
        data['lmd_wind_v'] = data['lmd_windspeed'] * np.sin(np.radians(data['lmd_winddirection']))
        
        # 5. NWPä¸LMDçš„å¯¹æ¯”ç‰¹å¾
        data['irrad_nwp_lmd_diff'] = data['nwp_globalirrad'] - data['lmd_totalirrad']
        data['temp_nwp_lmd_diff'] = data['nwp_temperature'] - data['lmd_temperature']
        data['pressure_nwp_lmd_diff'] = data['nwp_pressure'] - data['lmd_pressure']
        data['windspeed_nwp_lmd_diff'] = data['nwp_windspeed'] - data['lmd_windspeed']
        
        # NWPä¸LMDçš„ç›¸å…³æ€§ç‰¹å¾
        data['irrad_nwp_lmd_ratio'] = data['nwp_globalirrad'] / (data['lmd_totalirrad'] + 1e-6)
        data['temp_nwp_lmd_ratio'] = data['nwp_temperature'] / (data['lmd_temperature'] + 273.15)
        
        # 6. åŠŸç‡ç›¸å…³ç‰¹å¾
        # åŠŸç‡æ»åç‰¹å¾
        power_lag_periods = [1, 2, 3, 4, 8, 12, 24, 48, 96]
        for lag in power_lag_periods:
            data[f'power_lag_{lag}'] = data['power'].shift(lag)
        
        # åŠŸç‡æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        power_windows = [4, 8, 12, 24, 48]
        for window in power_windows:
            data[f'power_rolling_mean_{window}'] = data['power'].rolling(window=window).mean()
            data[f'power_rolling_std_{window}'] = data['power'].rolling(window=window).std()
            data[f'power_rolling_max_{window}'] = data['power'].rolling(window=window).max()
        
        # 7. ç™½å¤©åˆ¤æ–­å’Œå¤ªé˜³è§’åº¦ç‰¹å¾
        data['is_daytime'] = ((data['hour'] >= 6) & (data['hour'] <= 18)).astype(int)
        
        # å¤ªé˜³é«˜åº¦è§’è¿‘ä¼¼è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆï¼‰
        day_of_year = data['day_of_year']
        hour_angle = (data['hour'] - 12) * 15  # æ—¶è§’
        declination = 23.45 * np.sin(np.radians(360 * (284 + day_of_year) / 365))
        latitude = 38.0  # å‡è®¾çº¬åº¦ï¼ˆæ ¹æ®metadataè°ƒæ•´ï¼‰
        
        data['solar_elevation'] = np.arcsin(
            np.sin(np.radians(declination)) * np.sin(np.radians(latitude)) +
            np.cos(np.radians(declination)) * np.cos(np.radians(latitude)) * 
            np.cos(np.radians(hour_angle))
        )
        data['solar_elevation'] = np.degrees(data['solar_elevation'])
        data['solar_elevation'] = np.maximum(data['solar_elevation'], 0)  # è´Ÿå€¼è®¾ä¸º0
        
        # 8. å¤©æ°”çŠ¶å†µåˆ†ç±»ç‰¹å¾
        # åŸºäºNWPæ•°æ®çš„å¤©æ°”åˆ†ç±»
        data['weather_clear'] = ((data['nwp_globalirrad'] > 600) & 
                                (data['nwp_humidity'] < 70)).astype(int)
        data['weather_cloudy'] = ((data['nwp_globalirrad'] > 200) & 
                                 (data['nwp_globalirrad'] <= 600)).astype(int)
        data['weather_overcast'] = (data['nwp_globalirrad'] <= 200).astype(int)
        data['weather_high_humidity'] = (data['nwp_humidity'] > 80).astype(int)
        
        # 9. å¡«å……ç¼ºå¤±å€¼å’Œå¤„ç†å¼‚å¸¸å€¼
        data = data.fillna(0)
        data = data.replace([np.inf, -np.inf], 0)
        
        # å¤„ç†æå€¼
        for col in data.columns:
            if col not in ['date_time'] and data[col].dtype in ['float64', 'int64']:
                q99 = data[col].quantile(0.99)
                q01 = data[col].quantile(0.01)
                data[col] = data[col].clip(lower=q01, upper=q99)
        
        print(f"  æ€»ç‰¹å¾æ•°é‡: {len(data.columns) - 2}")  # å‡å»date_timeå’Œpower
        
        # åˆ†æç‰¹å¾é‡è¦æ€§ç±»åˆ«
        nwp_feature_count = len([col for col in data.columns if 'nwp_' in col])
        lmd_feature_count = len([col for col in data.columns if 'lmd_' in col])
        time_feature_count = len([col for col in data.columns if any(x in col for x in ['hour', 'day', 'month', 'time_slot', 'sin', 'cos'])])
        power_feature_count = len([col for col in data.columns if 'power_' in col])
        
        print(f"  NWPç›¸å…³ç‰¹å¾: {nwp_feature_count}")
        print(f"  LMDç›¸å…³ç‰¹å¾: {lmd_feature_count}")
        print(f"  æ—¶é—´ç›¸å…³ç‰¹å¾: {time_feature_count}")
        print(f"  åŠŸç‡å†å²ç‰¹å¾: {power_feature_count}")
        
        return data
    
    def split_data(self, data, test_days=7):
        """åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        print(f"ğŸ“Š åˆ†å‰²æ•°æ® - æœ€å{test_days}å¤©ä½œä¸ºæµ‹è¯•é›†...")
        
        test_start_idx = len(data) - test_days * 96
        
        train_data = data[:test_start_idx].copy()
        test_data = data[test_start_idx:].copy()
        
        print(f"  è®­ç»ƒé›†: {len(train_data):,} æ¡è®°å½•")
        print(f"  æµ‹è¯•é›†: {len(test_data):,} æ¡è®°å½•")
        
        return train_data, test_data
    
    def train_models(self, train_data):
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”"""
        print("ğŸš€ è®­ç»ƒå¤šä¸ªé¢„æµ‹æ¨¡å‹...")
        
        # åªä½¿ç”¨ç™½å¤©æ—¶æ®µçš„æ•°æ®è¿›è¡Œè®­ç»ƒ
        daytime_mask = train_data['is_daytime'] == 1
        train_subset = train_data[daytime_mask].copy()
        train_subset = train_subset.dropna()
        
        if len(train_subset) == 0:
            raise ValueError("æ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®")
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        feature_cols = [col for col in train_subset.columns 
                       if col not in ['date_time', 'power']]
        X_train = train_subset[feature_cols]
        y_train = train_subset['power']
        
        self.feature_names = feature_cols
        
        # 1. åŸºç¡€æ¨¡å‹ï¼ˆä¸ä½¿ç”¨NWPï¼‰
        print("  è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆä¸ä½¿ç”¨NWPï¼‰...")
        basic_features = [col for col in feature_cols if not col.startswith('nwp_')]
        X_train_basic = X_train[basic_features]
        
        self.models['basic'] = xgb.XGBRegressor(
            objective='reg:squarederror',
            max_depth=8,
            learning_rate=0.05,
            n_estimators=300,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=42,
            n_jobs=-1
        )
        self.models['basic'].fit(X_train_basic, y_train)
        
        # 2. NWPå¢å¼ºæ¨¡å‹ï¼ˆä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼‰
        print("  è®­ç»ƒNWPå¢å¼ºæ¨¡å‹...")
        self.models['nwp_enhanced'] = xgb.XGBRegressor(
            objective='reg:squarederror',
            max_depth=10,
            learning_rate=0.05,
            n_estimators=500,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=42,
            n_jobs=-1
        )
        self.models['nwp_enhanced'].fit(X_train, y_train)
        
        # 3. åªä½¿ç”¨NWPçš„æ¨¡å‹
        print("  è®­ç»ƒçº¯NWPæ¨¡å‹...")
        nwp_features = [col for col in feature_cols if 'nwp_' in col or 
                       col in ['hour', 'minute', 'day_of_week', 'month', 'is_daytime',
                              'hour_sin', 'hour_cos', 'solar_elevation']]
        X_train_nwp = X_train[nwp_features]
        
        self.models['nwp_only'] = xgb.XGBRegressor(
            objective='reg:squarederror',
            max_depth=8,
            learning_rate=0.05,
            n_estimators=400,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=42,
            n_jobs=-1
        )
        self.models['nwp_only'].fit(X_train_nwp, y_train)
        
        # 4. éšæœºæ£®æ—æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        print("  è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        self.models['random_forest'] = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        self.models['random_forest'].fit(X_train, y_train)
        
        # æ‰“å°è®­ç»ƒé›†æ€§èƒ½
        for model_name, model in self.models.items():
            if model_name == 'basic':
                train_pred = model.predict(X_train_basic)
            elif model_name == 'nwp_only':
                train_pred = model.predict(X_train_nwp)
            else:
                train_pred = model.predict(X_train)
            
            train_r2 = r2_score(y_train, train_pred)
            train_mae = mean_absolute_error(y_train, train_pred)
            print(f"    {model_name} - R^2: {train_r2:.4f}, MAE: {train_mae:.4f}")
    
    def predict_test_period(self, test_data):
        """ä½¿ç”¨ä¸åŒæ¨¡å‹é¢„æµ‹æµ‹è¯•æœŸé—´çš„åŠŸç‡"""
        print("ğŸ”® ä½¿ç”¨ä¸åŒæ¨¡å‹é¢„æµ‹æµ‹è¯•æœŸé—´åŠŸç‡...")
        
        predictions = {}
        
        for model_name, model in self.models.items():
            model_predictions = []
            
            for idx, row in test_data.iterrows():
                # å‡†å¤‡ç‰¹å¾
                if model_name == 'basic':
                    feature_cols = [col for col in self.feature_names if not col.startswith('nwp_')]
                elif model_name == 'nwp_only':
                    feature_cols = [col for col in self.feature_names if 'nwp_' in col or 
                                   col in ['hour', 'minute', 'day_of_week', 'month', 'is_daytime',
                                          'hour_sin', 'hour_cos', 'solar_elevation']]
                else:
                    feature_cols = self.feature_names
                
                features = row[feature_cols].values.reshape(1, -1)
                
                # é¢„æµ‹
                pred = model.predict(features)[0]
                pred = max(0, pred)
                
                # å¤œé—´æ—¶æ®µè®¾ä¸º0
                if row['is_daytime'] == 0:
                    pred = 0
                else:
                    # æ·»åŠ é€‚å½“çš„éšæœºæ‰°åŠ¨
                    noise_factor = np.random.normal(0, 0.02)
                    pred = pred * (1 + noise_factor)
                    pred = max(0, min(pred, self.capacity))
                
                model_predictions.append(pred)
            
            predictions[model_name] = np.array(model_predictions)
            print(f"  {model_name} é¢„æµ‹å®Œæˆ")
        
        return predictions
    
    def calculate_evaluation_metrics(self, actual, predicted):
        """è®¡ç®—è¯„ä»·æŒ‡æ ‡"""
        daytime_mask = (actual > 0) | (predicted > 0)
        
        if np.sum(daytime_mask) == 0:
            return {}
        
        actual_day = actual[daytime_mask]
        predicted_day = predicted[daytime_mask]
        
        # å½’ä¸€åŒ–è¯¯å·®
        normalized_actual = actual_day / self.capacity
        normalized_predicted = predicted_day / self.capacity
        normalized_error = normalized_predicted - normalized_actual
        
        rmse = np.sqrt(np.mean(normalized_error ** 2))
        mae = np.mean(np.abs(normalized_error))
        me = np.mean(normalized_error)
        
        if len(actual_day) > 1:
            correlation = np.corrcoef(actual_day, predicted_day)[0, 1]
        else:
            correlation = 0
        
        accuracy = (1 - rmse) * 100
        qualification_mask = np.abs(normalized_error) < 0.25
        qualification_rate = np.mean(qualification_mask) * 100
        
        return {
            'RMSE': rmse,
            'MAE': mae,
            'ME': me,
            'Correlation': correlation,
            'Accuracy': accuracy,
            'Qualification_Rate': qualification_rate,
            'Sample_Count': len(actual_day)
        }
    
    def analyze_nwp_effectiveness(self):
        """åˆ†æNWPä¿¡æ¯çš„æœ‰æ•ˆæ€§"""
        print("ğŸ“ˆ åˆ†æNWPä¿¡æ¯çš„æœ‰æ•ˆæ€§...")
        
        actual = self.test_data['power'].values
        
        # è®¡ç®—å„æ¨¡å‹çš„è¯„ä»·æŒ‡æ ‡
        for model_name, predictions in self.predictions.items():
            metrics = self.calculate_evaluation_metrics(actual, predictions)
            self.metrics[model_name] = metrics
        
        # å¯¹æ¯”åˆ†æ
        print("\n" + "="*60)
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ")
        print("="*60)
        
        for model_name, metrics in self.metrics.items():
            print(f"\n{model_name.upper()} æ¨¡å‹:")
            print(f"  RMSE: {metrics['RMSE']:.6f}")
            print(f"  MAE: {metrics['MAE']:.6f}")
            print(f"  ç›¸å…³ç³»æ•°: {metrics['Correlation']:.4f}")
            print(f"  å‡†ç¡®ç‡: {metrics['Accuracy']:.2f}%")
            print(f"  åˆæ ¼ç‡: {metrics['Qualification_Rate']:.2f}%")
        
        # NWPæœ‰æ•ˆæ€§åˆ†æ
        print(f"\n{'='*60}")
        print("ğŸ¯ NWPä¿¡æ¯æœ‰æ•ˆæ€§åˆ†æ")
        print("="*60)
        
        basic_rmse = self.metrics['basic']['RMSE']
        nwp_enhanced_rmse = self.metrics['nwp_enhanced']['RMSE']
        nwp_only_rmse = self.metrics['nwp_only']['RMSE']
        
        rmse_improvement = (basic_rmse - nwp_enhanced_rmse) / basic_rmse * 100
        
        print(f"åŸºç¡€æ¨¡å‹ vs NWPå¢å¼ºæ¨¡å‹:")
        print(f"  RMSEæ”¹å–„: {rmse_improvement:.2f}%")
        
        basic_acc = self.metrics['basic']['Accuracy']
        nwp_enhanced_acc = self.metrics['nwp_enhanced']['Accuracy']
        acc_improvement = nwp_enhanced_acc - basic_acc
        
        print(f"  å‡†ç¡®ç‡æå‡: {acc_improvement:.2f}ä¸ªç™¾åˆ†ç‚¹")
        
        # åˆ¤æ–­NWPæ˜¯å¦æœ‰æ•ˆ
        if rmse_improvement > 5 and acc_improvement > 2:
            effectiveness = "æ˜¾è‘—æœ‰æ•ˆ"
        elif rmse_improvement > 2 and acc_improvement > 1:
            effectiveness = "æœ‰æ•ˆ"
        elif rmse_improvement > 0:
            effectiveness = "è½»å¾®æœ‰æ•ˆ"
        else:
            effectiveness = "æ— æ•ˆæˆ–è´Ÿé¢å½±å“"
        
        print(f"\nğŸ¯ NWPä¿¡æ¯æœ‰æ•ˆæ€§è¯„ä»·: {effectiveness}")
        
        return effectiveness
    
    def identify_improvement_scenarios(self):
        """è¯†åˆ«NWPä¿¡æ¯æé«˜é¢„æµ‹ç²¾åº¦çš„åœºæ™¯"""
        print("\nğŸ” è¯†åˆ«NWPä¿¡æ¯æé«˜é¢„æµ‹ç²¾åº¦çš„åœºæ™¯...")
        
        # å‡†å¤‡æ•°æ®
        test_data_copy = self.test_data.copy()
        test_data_copy['actual_power'] = test_data_copy['power']
        test_data_copy['basic_pred'] = self.predictions['basic']
        test_data_copy['nwp_enhanced_pred'] = self.predictions['nwp_enhanced']
        
        # è®¡ç®—è¯¯å·®
        test_data_copy['basic_error'] = np.abs(test_data_copy['basic_pred'] - test_data_copy['actual_power'])
        test_data_copy['nwp_enhanced_error'] = np.abs(test_data_copy['nwp_enhanced_pred'] - test_data_copy['actual_power'])
        test_data_copy['error_improvement'] = test_data_copy['basic_error'] - test_data_copy['nwp_enhanced_error']
        
        # åªåˆ†æç™½å¤©æ—¶æ®µ
        daytime_data = test_data_copy[test_data_copy['is_daytime'] == 1].copy()
        
        if len(daytime_data) == 0:
            print("âŒ æ²¡æœ‰ç™½å¤©æ—¶æ®µæ•°æ®è¿›è¡Œåœºæ™¯åˆ†æ")
            return {}
        
        # åœºæ™¯åˆ†æ
        scenarios = {}
        
        # 1. å¤©æ°”æ¡ä»¶åœºæ™¯
        print("\n1ï¸âƒ£ å¤©æ°”æ¡ä»¶åœºæ™¯åˆ†æ:")
        
        # æ™´å¤©åœºæ™¯
        clear_mask = daytime_data['weather_clear'] == 1
        if clear_mask.sum() > 0:
            clear_improvement = daytime_data[clear_mask]['error_improvement'].mean()
            scenarios['clear_weather'] = {
                'improvement': clear_improvement,
                'sample_count': clear_mask.sum(),
                'description': 'æ™´å¤©åœºæ™¯'
            }
            print(f"  æ™´å¤©: å¹³å‡è¯¯å·®æ”¹å–„ {clear_improvement:.4f} MW ({clear_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # å¤šäº‘åœºæ™¯
        cloudy_mask = daytime_data['weather_cloudy'] == 1
        if cloudy_mask.sum() > 0:
            cloudy_improvement = daytime_data[cloudy_mask]['error_improvement'].mean()
            scenarios['cloudy_weather'] = {
                'improvement': cloudy_improvement,
                'sample_count': cloudy_mask.sum(),
                'description': 'å¤šäº‘åœºæ™¯'
            }
            print(f"  å¤šäº‘: å¹³å‡è¯¯å·®æ”¹å–„ {cloudy_improvement:.4f} MW ({cloudy_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # é˜´å¤©åœºæ™¯
        overcast_mask = daytime_data['weather_overcast'] == 1
        if overcast_mask.sum() > 0:
            overcast_improvement = daytime_data[overcast_mask]['error_improvement'].mean()
            scenarios['overcast_weather'] = {
                'improvement': overcast_improvement,
                'sample_count': overcast_mask.sum(),
                'description': 'é˜´å¤©åœºæ™¯'
            }
            print(f"  é˜´å¤©: å¹³å‡è¯¯å·®æ”¹å–„ {overcast_improvement:.4f} MW ({overcast_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # 2. è¾å°„å¼ºåº¦åœºæ™¯
        print("\n2ï¸âƒ£ è¾å°„å¼ºåº¦åœºæ™¯åˆ†æ:")
        
        # é«˜è¾å°„åœºæ™¯
        high_irrad_mask = daytime_data['nwp_globalirrad'] > daytime_data['nwp_globalirrad'].quantile(0.75)
        if high_irrad_mask.sum() > 0:
            high_irrad_improvement = daytime_data[high_irrad_mask]['error_improvement'].mean()
            scenarios['high_irradiance'] = {
                'improvement': high_irrad_improvement,
                'sample_count': high_irrad_mask.sum(),
                'description': 'é«˜è¾å°„åœºæ™¯'
            }
            print(f"  é«˜è¾å°„: å¹³å‡è¯¯å·®æ”¹å–„ {high_irrad_improvement:.4f} MW ({high_irrad_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # ä¸­ç­‰è¾å°„åœºæ™¯
        med_irrad_mask = ((daytime_data['nwp_globalirrad'] > daytime_data['nwp_globalirrad'].quantile(0.25)) & 
                         (daytime_data['nwp_globalirrad'] <= daytime_data['nwp_globalirrad'].quantile(0.75)))
        if med_irrad_mask.sum() > 0:
            med_irrad_improvement = daytime_data[med_irrad_mask]['error_improvement'].mean()
            scenarios['medium_irradiance'] = {
                'improvement': med_irrad_improvement,
                'sample_count': med_irrad_mask.sum(),
                'description': 'ä¸­ç­‰è¾å°„åœºæ™¯'
            }
            print(f"  ä¸­ç­‰è¾å°„: å¹³å‡è¯¯å·®æ”¹å–„ {med_irrad_improvement:.4f} MW ({med_irrad_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # ä½è¾å°„åœºæ™¯
        low_irrad_mask = daytime_data['nwp_globalirrad'] <= daytime_data['nwp_globalirrad'].quantile(0.25)
        if low_irrad_mask.sum() > 0:
            low_irrad_improvement = daytime_data[low_irrad_mask]['error_improvement'].mean()
            scenarios['low_irradiance'] = {
                'improvement': low_irrad_improvement,
                'sample_count': low_irrad_mask.sum(),
                'description': 'ä½è¾å°„åœºæ™¯'
            }
            print(f"  ä½è¾å°„: å¹³å‡è¯¯å·®æ”¹å–„ {low_irrad_improvement:.4f} MW ({low_irrad_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # 3. æ—¶é—´åœºæ™¯
        print("\n3ï¸âƒ£ æ—¶é—´åœºæ™¯åˆ†æ:")
        
        # ä¸Šåˆåœºæ™¯
        morning_mask = (daytime_data['hour'] >= 6) & (daytime_data['hour'] < 12)
        if morning_mask.sum() > 0:
            morning_improvement = daytime_data[morning_mask]['error_improvement'].mean()
            scenarios['morning'] = {
                'improvement': morning_improvement,
                'sample_count': morning_mask.sum(),
                'description': 'ä¸Šåˆæ—¶æ®µ'
            }
            print(f"  ä¸Šåˆ(6-12h): å¹³å‡è¯¯å·®æ”¹å–„ {morning_improvement:.4f} MW ({morning_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # ä¸‹åˆåœºæ™¯
        afternoon_mask = (daytime_data['hour'] >= 12) & (daytime_data['hour'] <= 18)
        if afternoon_mask.sum() > 0:
            afternoon_improvement = daytime_data[afternoon_mask]['error_improvement'].mean()
            scenarios['afternoon'] = {
                'improvement': afternoon_improvement,
                'sample_count': afternoon_mask.sum(),
                'description': 'ä¸‹åˆæ—¶æ®µ'
            }
            print(f"  ä¸‹åˆ(12-18h): å¹³å‡è¯¯å·®æ”¹å–„ {afternoon_improvement:.4f} MW ({afternoon_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # 4. æ¸©åº¦åœºæ™¯
        print("\n4ï¸âƒ£ æ¸©åº¦åœºæ™¯åˆ†æ:")
        
        # é«˜æ¸©åœºæ™¯
        high_temp_mask = daytime_data['nwp_temp_celsius'] > daytime_data['nwp_temp_celsius'].quantile(0.75)
        if high_temp_mask.sum() > 0:
            high_temp_improvement = daytime_data[high_temp_mask]['error_improvement'].mean()
            scenarios['high_temperature'] = {
                'improvement': high_temp_improvement,
                'sample_count': high_temp_mask.sum(),
                'description': 'é«˜æ¸©åœºæ™¯'
            }
            print(f"  é«˜æ¸©: å¹³å‡è¯¯å·®æ”¹å–„ {high_temp_improvement:.4f} MW ({high_temp_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # é€‚å®œæ¸©åº¦åœºæ™¯
        opt_temp_mask = daytime_data['nwp_temp_optimal'] < 5  # ä¸25Â°Cç›¸å·®å°äº5Â°C
        if opt_temp_mask.sum() > 0:
            opt_temp_improvement = daytime_data[opt_temp_mask]['error_improvement'].mean()
            scenarios['optimal_temperature'] = {
                'improvement': opt_temp_improvement,
                'sample_count': opt_temp_mask.sum(),
                'description': 'é€‚å®œæ¸©åº¦åœºæ™¯'
            }
            print(f"  é€‚å®œæ¸©åº¦: å¹³å‡è¯¯å·®æ”¹å–„ {opt_temp_improvement:.4f} MW ({opt_temp_mask.sum()}ä¸ªæ ·æœ¬)")
        
        # æ‰¾å‡ºæœ€æœ‰æ•ˆçš„åœºæ™¯
        print(f"\n{'='*60}")
        print("ğŸ† NWPä¿¡æ¯æœ€æœ‰æ•ˆçš„åœºæ™¯æ’å")
        print("="*60)
        
        sorted_scenarios = sorted(scenarios.items(), 
                                key=lambda x: x[1]['improvement'], reverse=True)
        
        for i, (scenario_name, scenario_data) in enumerate(sorted_scenarios[:5], 1):
            print(f"{i}. {scenario_data['description']}: "
                  f"è¯¯å·®æ”¹å–„ {scenario_data['improvement']:.4f} MW "
                  f"({scenario_data['sample_count']}ä¸ªæ ·æœ¬)")
        
        return scenarios
    
    def create_comprehensive_visualizations(self):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–åˆ†æ"""
        print("ğŸ“Š ç”Ÿæˆç»¼åˆå¯è§†åŒ–åˆ†æ...")
        
        self.ensure_chinese_font()
        
        fig_dir = Path("results/figures")
        fig_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
        self.plot_model_comparison()
        
        # 2. NWPç‰¹å¾é‡è¦æ€§åˆ†æ
        self.plot_nwp_feature_importance()
        
        # 3. åœºæ™¯åˆ†æå¯è§†åŒ–
        scenarios = self.identify_improvement_scenarios()
        self.plot_scenario_analysis(scenarios)
        
        # 4. æ—¶é—´åºåˆ—å¯¹æ¯”
        self.plot_time_series_comparison()
        
        print("âœ… å¯è§†åŒ–åˆ†æå®Œæˆ")
    
    def plot_model_comparison(self):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾"""
        self.ensure_chinese_font()
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'{self.station_id} ä¸åŒæ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        models = list(self.metrics.keys())
        
        # RMSEå¯¹æ¯”
        ax1 = axes[0, 0]
        rmse_values = [self.metrics[m]['RMSE'] for m in models]
        bars1 = ax1.bar(models, rmse_values, alpha=0.7, color=['red', 'blue', 'green', 'orange'])
        ax1.set_title('RMSEå¯¹æ¯” (è¶Šå°è¶Šå¥½)')
        ax1.set_ylabel('RMSE')
        ax1.grid(True, alpha=0.3)
        for bar, value in zip(bars1, rmse_values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'{value:.4f}', ha='center', va='bottom', fontsize=10)
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        ax2 = axes[0, 1]
        acc_values = [self.metrics[m]['Accuracy'] for m in models]
        bars2 = ax2.bar(models, acc_values, alpha=0.7, color=['red', 'blue', 'green', 'orange'])
        ax2.set_title('å‡†ç¡®ç‡å¯¹æ¯” (è¶Šå¤§è¶Šå¥½)')
        ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax2.grid(True, alpha=0.3)
        for bar, value in zip(bars2, acc_values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{value:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # ç›¸å…³ç³»æ•°å¯¹æ¯”
        ax3 = axes[1, 0]
        corr_values = [self.metrics[m]['Correlation'] for m in models]
        bars3 = ax3.bar(models, corr_values, alpha=0.7, color=['red', 'blue', 'green', 'orange'])
        ax3.set_title('ç›¸å…³ç³»æ•°å¯¹æ¯” (è¶Šå¤§è¶Šå¥½)')
        ax3.set_ylabel('ç›¸å…³ç³»æ•°')
        ax3.grid(True, alpha=0.3)
        for bar, value in zip(bars3, corr_values):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        # åˆæ ¼ç‡å¯¹æ¯”
        ax4 = axes[1, 1]
        qr_values = [self.metrics[m]['Qualification_Rate'] for m in models]
        bars4 = ax4.bar(models, qr_values, alpha=0.7, color=['red', 'blue', 'green', 'orange'])
        ax4.set_title('åˆæ ¼ç‡å¯¹æ¯” (è¶Šå¤§è¶Šå¥½)')
        ax4.set_ylabel('åˆæ ¼ç‡ (%)')
        ax4.grid(True, alpha=0.3)
        for bar, value in zip(bars4, qr_values):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{value:.1f}%', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(f'results/figures/{self.station_id}_model_comparison.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_nwp_feature_importance(self):
        """ç»˜åˆ¶NWPç‰¹å¾é‡è¦æ€§"""
        self.ensure_chinese_font()
        
        if hasattr(self.models['nwp_enhanced'], 'feature_importances_'):
            importances = self.models['nwp_enhanced'].feature_importances_
            feature_importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)
            
            # åˆ†ç±»ç‰¹å¾
            nwp_features = feature_importance_df[feature_importance_df['feature'].str.contains('nwp_')]
            lmd_features = feature_importance_df[feature_importance_df['feature'].str.contains('lmd_')]
            other_features = feature_importance_df[~feature_importance_df['feature'].str.contains('nwp_|lmd_')]
            
            fig, axes = plt.subplots(1, 3, figsize=(20, 8))
            fig.suptitle(f'{self.station_id} ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=16, fontweight='bold')
            
            # NWPç‰¹å¾é‡è¦æ€§
            ax1 = axes[0]
            top_nwp = nwp_features.head(15)
            sns.barplot(data=top_nwp, y='feature', x='importance', ax=ax1)
            ax1.set_title('NWPç‰¹å¾é‡è¦æ€§ (Top 15)')
            ax1.set_xlabel('é‡è¦æ€§å¾—åˆ†')
            
            # LMDç‰¹å¾é‡è¦æ€§
            ax2 = axes[1]
            top_lmd = lmd_features.head(10)
            sns.barplot(data=top_lmd, y='feature', x='importance', ax=ax2)
            ax2.set_title('LMDç‰¹å¾é‡è¦æ€§ (Top 10)')
            ax2.set_xlabel('é‡è¦æ€§å¾—åˆ†')
            
            # å…¶ä»–ç‰¹å¾é‡è¦æ€§
            ax3 = axes[2]
            top_other = other_features.head(10)
            sns.barplot(data=top_other, y='feature', x='importance', ax=ax3)
            ax3.set_title('å…¶ä»–ç‰¹å¾é‡è¦æ€§ (Top 10)')
            ax3.set_xlabel('é‡è¦æ€§å¾—åˆ†')
            
            plt.tight_layout()
            plt.savefig(f'results/figures/{self.station_id}_feature_importance.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
    
    def plot_scenario_analysis(self, scenarios):
        """ç»˜åˆ¶åœºæ™¯åˆ†æå›¾"""
        self.ensure_chinese_font()
        
        if not scenarios:
            return
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        scenario_names = [scenarios[k]['description'] for k in scenarios.keys()]
        improvements = [scenarios[k]['improvement'] for k in scenarios.keys()]
        sample_counts = [scenarios[k]['sample_count'] for k in scenarios.keys()]
        
        # åˆ›å»ºæ°”æ³¡å›¾
        colors = plt.cm.viridis(np.linspace(0, 1, len(scenario_names)))
        scatter = ax.scatter(range(len(scenario_names)), improvements, 
                           s=[c*10 for c in sample_counts], c=colors, alpha=0.7)
        
        ax.set_xticks(range(len(scenario_names)))
        ax.set_xticklabels(scenario_names, rotation=45, ha='right')
        ax.set_ylabel('è¯¯å·®æ”¹å–„ (MW)')
        ax.set_title(f'{self.station_id} NWPä¿¡æ¯åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§')
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (improvement, count) in enumerate(zip(improvements, sample_counts)):
            ax.text(i, improvement + 0.01, f'{improvement:.3f}\n({count}æ ·æœ¬)', 
                   ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(f'results/figures/{self.station_id}_scenario_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_time_series_comparison(self):
        """ç»˜åˆ¶æ—¶é—´åºåˆ—å¯¹æ¯”"""
        self.ensure_chinese_font()
        
        fig, axes = plt.subplots(2, 1, figsize=(16, 10))
        fig.suptitle(f'{self.station_id} é¢„æµ‹ç»“æœæ—¶é—´åºåˆ—å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        time_range = self.test_data['date_time']
        actual = self.test_data['power']
        
        # ä¸Šå›¾ï¼šæ‰€æœ‰æ¨¡å‹å¯¹æ¯”
        ax1 = axes[0]
        ax1.plot(time_range, actual, label='å®é™…åŠŸç‡', linewidth=2, alpha=0.8, color='black')
        
        colors = ['red', 'blue', 'green', 'orange']
        for i, (model_name, predictions) in enumerate(self.predictions.items()):
            ax1.plot(time_range, predictions, label=f'{model_name}é¢„æµ‹', 
                    linewidth=1.5, alpha=0.7, color=colors[i])
        
        ax1.set_title('æ‰€æœ‰æ¨¡å‹é¢„æµ‹å¯¹æ¯”')
        ax1.set_ylabel('åŠŸç‡ (MW)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ä¸‹å›¾ï¼šåŸºç¡€æ¨¡å‹vs NWPå¢å¼ºæ¨¡å‹
        ax2 = axes[1]
        ax2.plot(time_range, actual, label='å®é™…åŠŸç‡', linewidth=2, alpha=0.8, color='black')
        ax2.plot(time_range, self.predictions['basic'], label='åŸºç¡€æ¨¡å‹', 
                linewidth=1.5, alpha=0.7, color='red')
        ax2.plot(time_range, self.predictions['nwp_enhanced'], label='NWPå¢å¼ºæ¨¡å‹', 
                linewidth=1.5, alpha=0.7, color='blue')
        
        ax2.set_title('åŸºç¡€æ¨¡å‹ vs NWPå¢å¼ºæ¨¡å‹')
        ax2.set_xlabel('æ—¶é—´')
        ax2.set_ylabel('åŠŸç‡ (MW)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æ ¼å¼åŒ–xè½´
        for ax in axes:
            ax.xaxis.set_major_formatter(DateFormatter('%m-%d'))
            ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        plt.savefig(f'results/figures/{self.station_id}_time_series_comparison.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        results_df = pd.DataFrame({
            'date_time': self.test_data['date_time'],
            'actual_power': self.test_data['power']
        })
        
        for model_name, predictions in self.predictions.items():
            results_df[f'{model_name}_prediction'] = predictions
            results_df[f'{model_name}_error'] = predictions - self.test_data['power']
        
        results_df.to_csv(results_dir / f'{self.station_id}_nwp_prediction_results.csv', index=False)
        
        # ä¿å­˜æ¨¡å‹
        for model_name, model in self.models.items():
            joblib.dump(model, results_dir / f'{self.station_id}_{model_name}_model.pkl')
        
        # ä¿å­˜è¯„ä»·æŒ‡æ ‡
        metrics_df = pd.DataFrame(self.metrics).T
        metrics_df.to_csv(results_dir / f'{self.station_id}_nwp_metrics.csv')
        
        print(f"âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„NWPåˆ†ææµç¨‹"""
        print(f"ğŸ¯ å¼€å§‹ {self.station_id} NWPä¿¡æ¯èå…¥åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        df = self.load_and_preprocess_data()
        
        # 2. ç‰¹å¾å·¥ç¨‹
        data = self.create_features(df)
        
        # 3. åˆ†å‰²æ•°æ®
        self.train_data, self.test_data = self.split_data(data, test_days=7)
        
        # 4. è®­ç»ƒå¤šä¸ªæ¨¡å‹
        self.train_models(self.train_data)
        
        # 5. é¢„æµ‹
        self.predictions = self.predict_test_period(self.test_data)
        
        # 6. åˆ†æNWPæœ‰æ•ˆæ€§
        effectiveness = self.analyze_nwp_effectiveness()
        
        # 7. åœºæ™¯åˆ†æ
        scenarios = self.identify_improvement_scenarios()
        
        # 8. å¯è§†åŒ–
        self.create_comprehensive_visualizations()
        
        # 9. ä¿å­˜ç»“æœ
        self.save_results()
        
        return effectiveness, scenarios

def run_nwp_analysis():
    """è¿è¡ŒNWPä¿¡æ¯èå…¥åˆ†æ"""
    stations = ['station00', 'station04', 'station05', 'station09']
    all_results = {}
    
    print("ğŸš€ å¼€å§‹NWPä¿¡æ¯èå…¥çš„å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹åˆ†æ...")
    print("="*80)
    
    for station in stations:
        try:
            print(f"\n{'='*30} {station} {'='*30}")
            predictor = NWPPowerPrediction(station)
            effectiveness, scenarios = predictor.run_complete_analysis()
            
            all_results[station] = {
                'effectiveness': effectiveness,
                'scenarios': scenarios,
                'metrics': predictor.metrics
            }
            
        except Exception as e:
            print(f"âŒ {station} åˆ†æå¤±è´¥: {str(e)}")
            continue
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    if all_results:
        create_nwp_summary_report(all_results)
    
    print(f"\nğŸ‰ NWPä¿¡æ¯èå…¥åˆ†æå®Œæˆï¼")
    return all_results

def create_nwp_summary_report(all_results):
    """åˆ›å»ºNWPåˆ†æç»¼åˆæŠ¥å‘Š"""
    print(f"\n{'='*80}")
    print("ğŸ“Š NWPä¿¡æ¯èå…¥æ•ˆæœç»¼åˆæŠ¥å‘Š")
    print("="*80)
    
    # ç»Ÿè®¡æœ‰æ•ˆæ€§
    effectiveness_count = {}
    for station, results in all_results.items():
        effectiveness = results['effectiveness']
        effectiveness_count[effectiveness] = effectiveness_count.get(effectiveness, 0) + 1
        print(f"{station}: {effectiveness}")
    
    print(f"\nğŸ“ˆ æœ‰æ•ˆæ€§ç»Ÿè®¡:")
    for effectiveness, count in effectiveness_count.items():
        print(f"  {effectiveness}: {count}ä¸ªç«™ç‚¹")
    
    # ä¿å­˜ç»¼åˆæŠ¥å‘Š
    report = f"""
# NWPä¿¡æ¯èå…¥å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹ç»¼åˆåˆ†ææŠ¥å‘Š

## åˆ†ææ¦‚è¿°
æœ¬æŠ¥å‘Šåˆ†æäº†NWPï¼ˆæ•°å€¼å¤©æ°”é¢„æŠ¥ï¼‰ä¿¡æ¯å¯¹å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹ç²¾åº¦çš„å½±å“ã€‚

## ç«™ç‚¹åˆ†æç»“æœ

"""
    
    for station, results in all_results.items():
        report += f"""
### {station}
- **NWPæœ‰æ•ˆæ€§**: {results['effectiveness']}
- **æ¨¡å‹æ€§èƒ½å¯¹æ¯”**:
"""
        
        for model_name, metrics in results['metrics'].items():
            report += f"  - {model_name}: RMSE={metrics['RMSE']:.6f}, å‡†ç¡®ç‡={metrics['Accuracy']:.2f}%\n"
        
        if results['scenarios']:
            report += f"- **æœ€æœ‰æ•ˆåœºæ™¯**: {list(results['scenarios'].keys())[:3]}\n"
    
    report += f"""
## æ€»ä½“ç»“è®º

### NWPä¿¡æ¯æœ‰æ•ˆæ€§ç»Ÿè®¡
"""
    
    for effectiveness, count in effectiveness_count.items():
        report += f"- {effectiveness}: {count}ä¸ªç«™ç‚¹\n"
    
    report += f"""
### å»ºè®®
1. å¯¹äºNWPä¿¡æ¯æ˜¾è‘—æœ‰æ•ˆçš„ç«™ç‚¹ï¼Œå»ºè®®åœ¨å®é™…åº”ç”¨ä¸­é‡‡ç”¨NWPå¢å¼ºæ¨¡å‹
2. å¯¹äºNWPä¿¡æ¯æœ‰æ•ˆçš„ç«™ç‚¹ï¼Œå¯åœ¨ç‰¹å®šåœºæ™¯ä¸‹ä½¿ç”¨NWPä¿¡æ¯
3. å¯¹äºNWPä¿¡æ¯æ— æ•ˆçš„ç«™ç‚¹ï¼Œå»ºè®®ä¼˜åŒ–NWPæ•°æ®è´¨é‡æˆ–ç‰¹å¾å·¥ç¨‹æ–¹æ³•

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    with open('results/nwp_comprehensive_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ… ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: results/nwp_comprehensive_report.md")

if __name__ == "__main__":
    # è¿è¡ŒNWPä¿¡æ¯èå…¥åˆ†æ
    results = run_nwp_analysis() 