# å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹è¯„ä»·æŒ‡æ ‡è®¡ç®— + å¯è§†åŒ–
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç»˜å›¾æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class PowerPredictionEvaluator:
    """å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹è¯„ä»·æŒ‡æ ‡è®¡ç®—å™¨ + å¯è§†åŒ–"""
    
    def __init__(self, capacity):
        """
        åˆå§‹åŒ–è¯„ä»·å™¨
        
        Args:
            capacity: å¼€æœºå®¹é‡ (MW)
        """
        self.capacity = capacity
    
    def calculate_metrics(self, actual_power, predicted_power, only_daytime=True):
        """
        è®¡ç®—è¯„ä»·æŒ‡æ ‡
        
        Args:
            actual_power: å®é™…åŠŸç‡æ•°ç»„
            predicted_power: é¢„æµ‹åŠŸç‡æ•°ç»„
            only_daytime: æ˜¯å¦åªè®¡ç®—ç™½å¤©æ—¶æ®µæŒ‡æ ‡
            
        Returns:
            dict: åŒ…å«æ‰€æœ‰è¯„ä»·æŒ‡æ ‡çš„å­—å…¸
        """
        # ç¡®ä¿æ•°ç»„é•¿åº¦ä¸€è‡´
        min_len = min(len(actual_power), len(predicted_power))
        actual = np.array(actual_power[:min_len])
        predicted = np.array(predicted_power[:min_len])
        
        if only_daytime:
            # åªè®¡ç®—ç™½å¤©æ—¶æ®µï¼ˆåŠŸç‡å¤§äº0çš„æ—¶æ®µï¼‰
            daytime_mask = (actual > 0) | (predicted > 0)
            if np.sum(daytime_mask) == 0:
                return {}
            actual = actual[daytime_mask]
            predicted = predicted[daytime_mask]
        
        n = len(actual)
        
        # å½’ä¸€åŒ–è¯¯å·®ï¼ˆç›¸å¯¹äºå¼€æœºå®¹é‡ï¼‰
        normalized_actual = actual / self.capacity
        normalized_predicted = predicted / self.capacity
        normalized_error = normalized_predicted - normalized_actual
        
        # 1. å‡æ–¹æ ¹è¯¯å·® (RMSE)
        rmse = np.sqrt(np.mean(normalized_error ** 2))
        
        # 2. å¹³å‡ç»å¯¹è¯¯å·® (MAE)
        mae = np.mean(np.abs(normalized_error))
        
        # 3. å¹³å‡è¯¯å·® (ME)
        me = np.mean(normalized_error)
        
        # 4. ç›¸å…³ç³»æ•° (r)
        if n > 1 and np.std(actual) > 0 and np.std(predicted) > 0:
            correlation = np.corrcoef(actual, predicted)[0, 1]
        else:
            correlation = 0
        
        # 5. å‡†ç¡®ç‡ (CR)
        accuracy = (1 - rmse) * 100
        
        # 6. åˆæ ¼ç‡ (QR) - è¯¯å·®å°äº25%çš„æ¯”ä¾‹
        qualified_mask = np.abs(normalized_error) < 0.25
        qualification_rate = np.sum(qualified_mask) / n * 100
        
        return {
            'RMSE': rmse,
            'MAE': mae,
            'ME': me,
            'Correlation': correlation,
            'Accuracy': accuracy,
            'Qualification_Rate': qualification_rate,
            'Sample_Count': n
        }
    
    def create_evaluation_visualization(self, actual, predicted, station_id, save_dir="results/figures"):
        """åˆ›å»ºè¯„ä»·å¯è§†åŒ–å›¾è¡¨"""
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç»¼åˆè¯„ä»·å›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'{station_id} é¢„æµ‹æ€§èƒ½è¯¦ç»†è¯„ä»·', fontsize=16, fontweight='bold')
        
        # 1. æ—¶é—´åºåˆ—å¯¹æ¯”
        ax1 = axes[0, 0]
        time_index = range(len(actual))
        ax1.plot(time_index, actual, label='å®é™…åŠŸç‡', alpha=0.8, linewidth=1.5)
        ax1.plot(time_index, predicted, label='é¢„æµ‹åŠŸç‡', alpha=0.8, linewidth=1.5)
        ax1.set_title('é¢„æµ‹vså®é™…åŠŸç‡å¯¹æ¯”')
        ax1.set_xlabel('æ—¶é—´ç‚¹')
        ax1.set_ylabel('åŠŸç‡ (MW)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æ•£ç‚¹å›¾
        ax2 = axes[0, 1]
        ax2.scatter(actual, predicted, alpha=0.6, s=20)
        max_val = max(actual.max(), predicted.max())
        ax2.plot([0, max_val], [0, max_val], 'r--', label='ç†æƒ³é¢„æµ‹çº¿')
        ax2.set_title('é¢„æµ‹vså®é™…æ•£ç‚¹å›¾')
        ax2.set_xlabel('å®é™…åŠŸç‡ (MW)')
        ax2.set_ylabel('é¢„æµ‹åŠŸç‡ (MW)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. è¯¯å·®åˆ†å¸ƒ
        ax3 = axes[0, 2]
        errors = predicted - actual
        ax3.hist(errors, bins=30, alpha=0.7, edgecolor='black')
        ax3.axvline(errors.mean(), color='red', linestyle='--', 
                   label=f'å¹³å‡è¯¯å·®: {errors.mean():.3f}')
        ax3.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
        ax3.set_xlabel('é¢„æµ‹è¯¯å·® (MW)')
        ax3.set_ylabel('é¢‘æ¬¡')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ
        ax4 = axes[1, 0]
        relative_errors = (predicted - actual) / self.capacity * 100
        ax4.hist(relative_errors, bins=30, alpha=0.7, edgecolor='black', color='orange')
        ax4.axvline(relative_errors.mean(), color='red', linestyle='--',
                   label=f'å¹³å‡ç›¸å¯¹è¯¯å·®: {relative_errors.mean():.2f}%')
        ax4.set_title('ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ (ç›¸å¯¹äºå¼€æœºå®¹é‡)')
        ax4.set_xlabel('ç›¸å¯¹è¯¯å·® (%)')
        ax4.set_ylabel('é¢‘æ¬¡')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. ç´¯ç§¯è¯¯å·®
        ax5 = axes[1, 1]
        cumulative_error = np.cumsum(errors)
        ax5.plot(time_index, cumulative_error, linewidth=2)
        ax5.set_title('ç´¯ç§¯è¯¯å·®è¶‹åŠ¿')
        ax5.set_xlabel('æ—¶é—´ç‚¹')
        ax5.set_ylabel('ç´¯ç§¯è¯¯å·® (MW)')
        ax5.grid(True, alpha=0.3)
        
        # 6. è¯¯å·®ç®±çº¿å›¾
        ax6 = axes[1, 2]
        error_data = [errors]
        ax6.boxplot(error_data, labels=[station_id])
        ax6.set_title('é¢„æµ‹è¯¯å·®ç®±çº¿å›¾')
        ax6.set_ylabel('é¢„æµ‹è¯¯å·® (MW)')
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_dir / f'{station_id}_detailed_evaluation.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_metrics_comparison_chart(self, metrics_dict, save_dir="results/figures"):
        """åˆ›å»ºå¤šç«™ç‚¹æŒ‡æ ‡å¯¹æ¯”å›¾è¡¨"""
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        stations = list(metrics_dict.keys())
        
        # åˆ›å»ºæŒ‡æ ‡å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('å¤šç«™ç‚¹é¢„æµ‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # 1. RMSEå¯¹æ¯”
        ax1 = axes[0, 0]
        rmse_values = [metrics_dict[s]['RMSE'] for s in stations]
        bars1 = ax1.bar(stations, rmse_values, alpha=0.7, color='red')
        ax1.set_title('å‡æ–¹æ ¹è¯¯å·® (RMSE)')
        ax1.set_ylabel('RMSE')
        ax1.grid(True, alpha=0.3)
        for bar, value in zip(bars1, rmse_values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'{value:.4f}', ha='center', va='bottom', fontsize=10)
        
        # 2. MAEå¯¹æ¯”
        ax2 = axes[0, 1]
        mae_values = [metrics_dict[s]['MAE'] for s in stations]
        bars2 = ax2.bar(stations, mae_values, alpha=0.7, color='orange')
        ax2.set_title('å¹³å‡ç»å¯¹è¯¯å·® (MAE)')
        ax2.set_ylabel('MAE')
        ax2.grid(True, alpha=0.3)
        for bar, value in zip(bars2, mae_values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'{value:.4f}', ha='center', va='bottom', fontsize=10)
        
        # 3. ç›¸å…³ç³»æ•°å¯¹æ¯”
        ax3 = axes[0, 2]
        corr_values = [metrics_dict[s]['Correlation'] for s in stations]
        bars3 = ax3.bar(stations, corr_values, alpha=0.7, color='blue')
        ax3.set_title('ç›¸å…³ç³»æ•°')
        ax3.set_ylabel('ç›¸å…³ç³»æ•°')
        ax3.grid(True, alpha=0.3)
        for bar, value in zip(bars3, corr_values):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        # 4. å‡†ç¡®ç‡å¯¹æ¯”
        ax4 = axes[1, 0]
        acc_values = [metrics_dict[s]['Accuracy'] for s in stations]
        bars4 = ax4.bar(stations, acc_values, alpha=0.7, color='green')
        ax4.set_title('å‡†ç¡®ç‡ (%)')
        ax4.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax4.grid(True, alpha=0.3)
        for bar, value in zip(bars4, acc_values):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{value:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # 5. åˆæ ¼ç‡å¯¹æ¯”
        ax5 = axes[1, 1]
        qr_values = [metrics_dict[s]['Qualification_Rate'] for s in stations]
        bars5 = ax5.bar(stations, qr_values, alpha=0.7, color='purple')
        ax5.set_title('åˆæ ¼ç‡ (%)')
        ax5.set_ylabel('åˆæ ¼ç‡ (%)')
        ax5.grid(True, alpha=0.3)
        for bar, value in zip(bars5, qr_values):
            ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{value:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # 6. æ ·æœ¬æ•°å¯¹æ¯”
        ax6 = axes[1, 2]
        sample_values = [metrics_dict[s]['Sample_Count'] for s in stations]
        bars6 = ax6.bar(stations, sample_values, alpha=0.7, color='brown')
        ax6.set_title('æ ·æœ¬æ•°é‡')
        ax6.set_ylabel('æ ·æœ¬æ•°')
        ax6.grid(True, alpha=0.3)
        for bar, value in zip(bars6, sample_values):
            ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                    f'{value}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'multi_station_metrics_comparison.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_radar_chart(self, metrics, station_id, save_dir="results/figures"):
        """åˆ›å»ºé›·è¾¾å›¾"""
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        categories = ['å‡†ç¡®ç‡', 'åˆæ ¼ç‡', 'ç›¸å…³ç³»æ•°', 'RMSE(å)', 'MAE(å)', 'ME(å)']
        values = [
            metrics['Accuracy'],
            metrics['Qualification_Rate'],
            metrics['Correlation'] * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            (1 - metrics['RMSE']) * 100,   # åå‘ï¼Œè¶Šå¤§è¶Šå¥½
            (1 - metrics['MAE']) * 100,    # åå‘ï¼Œè¶Šå¤§è¶Šå¥½
            (1 - abs(metrics['ME'])) * 100  # åå‘ï¼Œè¶Šå¤§è¶Šå¥½
        ]
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
        values += values[:1]  # é—­åˆå›¾å½¢
        angles = np.concatenate((angles, [angles[0]]))
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        ax.plot(angles, values, 'o-', linewidth=2, label=station_id)
        ax.fill(angles, values, alpha=0.25)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 100)
        ax.set_title(f'{station_id} é¢„æµ‹æ€§èƒ½é›·è¾¾å›¾', size=16, fontweight='bold', pad=20)
        ax.grid(True)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for angle, value, category in zip(angles[:-1], values[:-1], categories):
            ax.text(angle, value + 5, f'{value:.1f}', ha='center', va='center', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(save_dir / f'{station_id}_radar_chart.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()

def evaluate_station_predictions():
    """è¯„ä¼°æ‰€æœ‰ç«™ç‚¹çš„é¢„æµ‹ç»“æœ"""
    print("ğŸ” å¼€å§‹è¯„ä¼°ç«™ç‚¹é¢„æµ‹ç»“æœ...")
    
    stations = ['station00', 'station04', 'station05', 'station09']
    all_metrics = {}
    
    # å¼€æœºå®¹é‡è®¾å®šï¼ˆæ ¹æ®å†å²æœ€å¤§åŠŸç‡ä¼°ç®—ï¼‰
    capacities = {
        'station00': 6.628,
        'station04': 32.122,
        'station05': 42.142,
        'station09': 14.454
    }
    
    for station in stations:
        try:
            print(f"\n{'='*20} è¯„ä¼° {station} {'='*20}")
            
            # è¯»å–é¢„æµ‹ç»“æœ
            results_file = f'results/{station}_prediction_results.csv'
            if not Path(results_file).exists():
                print(f"âŒ æ‰¾ä¸åˆ° {station} çš„é¢„æµ‹ç»“æœæ–‡ä»¶")
                continue
            
            results_df = pd.read_csv(results_file)
            actual_power = results_df['actual_power'].values
            predicted_power = results_df['predicted_power'].values
            
            # åˆ›å»ºè¯„ä»·å™¨
            evaluator = PowerPredictionEvaluator(capacities[station])
            
            # è®¡ç®—è¯„ä»·æŒ‡æ ‡
            metrics = evaluator.calculate_metrics(actual_power, predicted_power)
            
            if not metrics:
                print(f"âŒ {station} æ²¡æœ‰æœ‰æ•ˆçš„ç™½å¤©æ—¶æ®µæ•°æ®")
                continue
            
            all_metrics[station] = metrics
            
            # æ‰“å°è¯„ä»·æŒ‡æ ‡
            print(f"\nğŸ“Š {station} è¯„ä»·æŒ‡æ ‡")
            print("="*50)
            print(f"æ ·æœ¬æ•°é‡: {metrics['Sample_Count']}")
            print(f"å¼€æœºå®¹é‡: {capacities[station]} MW")
            print(f"å®é™…åŠŸç‡å‡å€¼: {actual_power[actual_power > 0].mean():.3f} MW")
            print(f"é¢„æµ‹åŠŸç‡å‡å€¼: {predicted_power[predicted_power > 0].mean():.3f} MW")
            print(f"å®é™…åŠŸç‡æœ€å¤§å€¼: {actual_power.max():.3f} MW")
            print(f"é¢„æµ‹åŠŸç‡æœ€å¤§å€¼: {predicted_power.max():.3f} MW")
            print("-" * 50)
            print(f"1. å‡æ–¹æ ¹è¯¯å·® (RMSE): {metrics['RMSE']:.6f}")
            print(f"2. å¹³å‡ç»å¯¹è¯¯å·® (MAE): {metrics['MAE']:.6f}")
            print(f"3. å¹³å‡è¯¯å·® (ME): {metrics['ME']:.6f}")
            print(f"4. ç›¸å…³ç³»æ•° (r): {metrics['Correlation']:.6f}")
            print(f"5. å‡†ç¡®ç‡ (CR): {metrics['Accuracy']:.2f}%")
            print(f"6. åˆæ ¼ç‡ (QR): {metrics['Qualification_Rate']:.2f}%")
            
            # åˆ›å»ºå¯è§†åŒ–
            evaluator.create_evaluation_visualization(
                actual_power, predicted_power, station
            )
            evaluator.create_radar_chart(metrics, station)
            
            # ä¿å­˜è¯„ä»·æŠ¥å‘Š
            save_evaluation_report(station, metrics, capacities[station], 
                                 actual_power, predicted_power)
            
        except Exception as e:
            print(f"âŒ è¯„ä¼° {station} æ—¶å‡ºé”™: {str(e)}")
            continue
    
    # åˆ›å»ºç»¼åˆå¯¹æ¯”
    if all_metrics:
        create_comprehensive_comparison(all_metrics)
        
        # åˆ›å»ºç»¼åˆå¯è§†åŒ–
        evaluator = PowerPredictionEvaluator(1.0)  # ä¸´æ—¶åˆ›å»ºç”¨äºå¯è§†åŒ–
        evaluator.create_metrics_comparison_chart(all_metrics)
    
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ è¯„ä¼°æŠ¥å‘Šä¿å­˜åœ¨ results/ ç›®å½•ä¸‹")
    
    return all_metrics

def save_evaluation_report(station_id, metrics, capacity, actual_power, predicted_power):
    """ä¿å­˜è¯„ä»·æŠ¥å‘Š"""
    report = f"""
# {station_id} å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹è¯„ä»·æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **ç«™ç‚¹**: {station_id}
- **å¼€æœºå®¹é‡**: {capacity:.3f} MW
- **è¯„ä¼°æ ·æœ¬æ•°**: {metrics['Sample_Count']}
- **å®é™…åŠŸç‡å‡å€¼**: {actual_power[actual_power > 0].mean():.3f} MW
- **é¢„æµ‹åŠŸç‡å‡å€¼**: {predicted_power[predicted_power > 0].mean():.3f} MW
- **å®é™…åŠŸç‡æœ€å¤§å€¼**: {actual_power.max():.3f} MW
- **é¢„æµ‹åŠŸç‡æœ€å¤§å€¼**: {predicted_power.max():.3f} MW

## è¯„ä»·æŒ‡æ ‡

### 1. å‡æ–¹æ ¹è¯¯å·® (RMSE)
- **æ•°å€¼**: {metrics['RMSE']:.6f}
- **è¯´æ˜**: é¢„æµ‹è¯¯å·®çš„å‡æ–¹æ ¹ï¼Œè¶Šå°è¶Šå¥½

### 2. å¹³å‡ç»å¯¹è¯¯å·® (MAE)
- **æ•°å€¼**: {metrics['MAE']:.6f}
- **è¯´æ˜**: é¢„æµ‹è¯¯å·®çš„å¹³å‡ç»å¯¹å€¼ï¼Œè¶Šå°è¶Šå¥½

### 3. å¹³å‡è¯¯å·® (ME)
- **æ•°å€¼**: {metrics['ME']:.6f}
- **è¯´æ˜**: é¢„æµ‹è¯¯å·®çš„å¹³å‡å€¼ï¼Œæ¥è¿‘0æœ€å¥½

### 4. ç›¸å…³ç³»æ•° (r)
- **æ•°å€¼**: {metrics['Correlation']:.6f}
- **è¯´æ˜**: é¢„æµ‹å€¼ä¸å®é™…å€¼çš„çº¿æ€§ç›¸å…³ç¨‹åº¦ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½

### 5. å‡†ç¡®ç‡ (CR)
- **æ•°å€¼**: {metrics['Accuracy']:.2f}%
- **è¯´æ˜**: åŸºäºRMSEè®¡ç®—çš„å‡†ç¡®ç‡ï¼Œè¶Šå¤§è¶Šå¥½

### 6. åˆæ ¼ç‡ (QR)
- **æ•°å€¼**: {metrics['Qualification_Rate']:.2f}%
- **è¯´æ˜**: ç›¸å¯¹è¯¯å·®å°äº25%çš„æ ·æœ¬æ¯”ä¾‹ï¼Œè¶Šå¤§è¶Šå¥½

## æ€§èƒ½è¯„ä»·

"""
    
    # æ€§èƒ½ç­‰çº§è¯„ä»·
    if metrics['Accuracy'] >= 80:
        performance_level = "ä¼˜ç§€"
    elif metrics['Accuracy'] >= 70:
        performance_level = "è‰¯å¥½"
    elif metrics['Accuracy'] >= 60:
        performance_level = "ä¸€èˆ¬"
    else:
        performance_level = "éœ€è¦æ”¹è¿›"
    
    report += f"- **æ•´ä½“æ€§èƒ½ç­‰çº§**: {performance_level}\n"
    report += f"- **å‡†ç¡®ç‡è¯„ä»·**: {metrics['Accuracy']:.2f}%\n"
    report += f"- **åˆæ ¼ç‡è¯„ä»·**: {metrics['Qualification_Rate']:.2f}%\n"
    
    # ä¿å­˜æŠ¥å‘Š
    with open(f'results/{station_id}_evaluation_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… è¯„ä»·æŠ¥å‘Šå·²ä¿å­˜: results\\{station_id}_evaluation_report.md")

def create_comprehensive_comparison(all_metrics):
    """åˆ›å»ºç»¼åˆå¯¹æ¯”æŠ¥å‘Š"""
    print(f"\n{'='*60}")
    print("ğŸ“Š å¤šç«™ç‚¹é¢„æµ‹æ€§èƒ½ç»¼åˆå¯¹æ¯”")
    print("="*60)
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    comparison_data = []
    for station, metrics in all_metrics.items():
        comparison_data.append({
            'ç«™ç‚¹': station,
            'RMSE': f"{metrics['RMSE']:.6f}",
            'MAE': f"{metrics['MAE']:.6f}",
            'ME': f"{metrics['ME']:.6f}",
            'ç›¸å…³ç³»æ•°': f"{metrics['Correlation']:.4f}",
            'å‡†ç¡®ç‡(%)': f"{metrics['Accuracy']:.2f}",
            'åˆæ ¼ç‡(%)': f"{metrics['Qualification_Rate']:.2f}",
            'æ ·æœ¬æ•°': metrics['Sample_Count']
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_df.to_csv('results/multi_station_evaluation_comparison.csv', index=False)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆä½¿ç”¨ç®€å•çš„è¡¨æ ¼æ ¼å¼ï¼‰
    table_str = comparison_df.to_string(index=False)
    
    report = f"""
# å¤šç«™ç‚¹å…‰ä¼å‘ç”µåŠŸç‡é¢„æµ‹æ€§èƒ½ç»¼åˆå¯¹æ¯”æŠ¥å‘Š

## å¯¹æ¯”è¡¨æ ¼

```
{table_str}
```

## æ€§èƒ½æ’å

### RMSEæ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰
"""
    
    # RMSEæ’å
    rmse_ranking = sorted(all_metrics.items(), key=lambda x: x[1]['RMSE'])
    for i, (station, metrics) in enumerate(rmse_ranking, 1):
        report += f"{i}. {station}: {metrics['RMSE']:.6f}\n"
    
    report += "\n### ç›¸å…³ç³»æ•°æ’åï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰\n"
    
    # ç›¸å…³ç³»æ•°æ’å
    corr_ranking = sorted(all_metrics.items(), key=lambda x: x[1]['Correlation'], reverse=True)
    for i, (station, metrics) in enumerate(corr_ranking, 1):
        report += f"{i}. {station}: {metrics['Correlation']:.4f}\n"
    
    report += "\n### åˆæ ¼ç‡æ’åï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰\n"
    
    # åˆæ ¼ç‡æ’å
    qr_ranking = sorted(all_metrics.items(), key=lambda x: x[1]['Qualification_Rate'], reverse=True)
    for i, (station, metrics) in enumerate(qr_ranking, 1):
        report += f"{i}. {station}: {metrics['Qualification_Rate']:.2f}%\n"
    
    report += f"""
## æ€»ä½“è¯„ä»·

- **æœ€ä½³RMSE**: {rmse_ranking[0][0]} ({rmse_ranking[0][1]['RMSE']:.6f})
- **æœ€ä½³ç›¸å…³æ€§**: {corr_ranking[0][0]} ({corr_ranking[0][1]['Correlation']:.4f})
- **æœ€ä½³åˆæ ¼ç‡**: {qr_ranking[0][0]} ({qr_ranking[0][1]['Qualification_Rate']:.2f}%)

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    # ä¿å­˜ç»¼åˆæŠ¥å‘Š
    with open('results/comprehensive_evaluation_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ… ç»¼åˆå¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: results/comprehensive_evaluation_report.md")

if __name__ == "__main__":
    # è¯„ä¼°æ‰€æœ‰ç«™ç‚¹çš„é¢„æµ‹ç»“æœ
    metrics = evaluate_station_predictions() 